{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to IKPyKit","text":""},{"location":"index.html#about-the-project","title":"About The Project","text":"<p>IKPyKit (Python for Isolation Kernel Toolkit) is an intuitive Python library designed for a variety of machine learning tasks including kernel similarity calculation, anomaly detection, clustering, and change detection\u2014all powered by the innovative Isolation Kernel (IK) . Isolation Kernel is a data-dependent kernel that measures similarity by isolating data points using an isolation mechanism. It uniquely adapts to the data distribution, with the property that points in sparse regions are more similar than those in dense regions. Notably, it requires no learning or closed-form expression, making it efficient and scalable.</p>"},{"location":"index.html#why-use-isolation-kernel","title":"Why use Isolation Kernel?","text":"<ul> <li>Data-Dependent Similarity: Unlike traditional kernels (e.g., Gaussian, Laplacian), Isolation Kernel adapts to the structure of the data rather than assuming a fixed similarity function.</li> <li>Handles Sparse and Dense Regions: Isolation Kernel effectively accounts for varying data densities, making it ideal for datasets with non-uniform distributions.</li> <li>No Learning Required: It eliminates the need for training or parameter tuning, simplifying implementation while reducing computational cost.</li> <li>Effective in High Dimensions: It uniquely addresses the curse of dimensionality, being the only known measure capable of finding exact nearest neighbors in high-dimensional spaces.</li> <li>Versatile Applications: Isolation Kernel has been successfully applied to tasks like anomaly detection, clustering, and processing stream data, graph data, trajectory data, and more.</li> </ul> <p>Learn more about its history and development on the IsolationKernel GitHub page.</p>"},{"location":"index.html#why-use-ikpykit","title":"Why use IKPyKit?","text":"<p>IKPyKit is specifically built to harness the power of Isolation Kernel, providing specialized algorithms for a wide range of data types and tasks. Its seamless integration with the scikit-learn API allows easy adoption and compatibility with scikit-learn tools.</p> <ul> <li>Tailored for Isolation Kernel: IKPyKit directly leverages the unique properties of Isolation Kernel for efficient and effective machine learning solutions.</li> <li>Efficient and User-Friendly: Designed for simplicity and performance, IKPyKit offers an intuitive interface built on the scikit-learn API.</li> <li>Support for Diverse Data Types: It supports graph data, group data, stream data, time series, and trajectory data, making it versatile for various domains.</li> <li>Comprehensive Resources: Users benefit from rich documentation and examples to quickly understand and apply the library\u2019s features.</li> <li>Ideal for Research and Industry: IKPyKit is suitable for both academic research and industrial applications, providing scalable and cutting-edge tools for modern machine learning challenges.</li> </ul>"},{"location":"index.html#installation-dependencies","title":"Installation &amp; Dependencies","text":"<p>To install the basic version of <code>IKPyKit</code> with core dependencies, run the following:</p> <pre><code>pip install ikpykit\n</code></pre> <p>For more installation options, including dependencies and additional features, check out our Installation Guide.</p>"},{"location":"index.html#example","title":"Example","text":"<pre><code># Anomaly Detection using inne.\nimport numpy as np\nfrom ikpykit.anomaly import INNE\nX = np.array([[-1.1, 0.2], [0.3, 0.5], [0.5, 1.1], [100, 90]])\nclf = INNE(contamination=0.25).fit(X)\nclf.predict([[0.1, 0.3], [0, 0.7], [90, 85]])\n</code></pre>"},{"location":"index.html#implemented-algorithms","title":"Implemented Algorithms","text":""},{"location":"index.html#summary","title":"Summary","text":"Algorithms Kernel Similarity Anomaly Detection Clustering Change Detection Point Data IsoKernel (AAAI'19, SIGKDD'18) IForest (ICDM'08, TKDD'12) IDKC (IS'23) INNE (CIJ'18) PSKC (TKDE'23) IDKD (TKDE'22) IKAHC (PRJ'23) Graph Data IsoGraphKernel (AAAI'21) IKGOD (SIAM'23) Group Data IsodisKernel (SIGKDD'20) IKGAD (TKDE'22) Stream Data StreaKHC (SIGKDD'22) ICID (JAIR'24) Time Series IKTOD (VLDB'22) Trajectory Data IKAT (JAIR'24) TIDKC (ICDM'23) <p>(i) Isolation Kernel :</p> Abbr Algorithm Application Publication IsoKernel Isolation Kernel IK feature mapping and similarity calculating AAAI2019, SIGKDD2018 IsoDisKernel Isolation Distribution Kernel Distribution similarity calculating SIGKDD2020 <p>(ii) Point Anomaly detection :</p> Abbr Algorithm Application Publication IForest Isolation forest Anomaly Detection ICDM2008, TKDD2022 INNE Isolation-based anomaly detection using nearest-neighbor ensembles Anomaly Detection CIJ2018 IDKD Isolation Distributional Kernel for point anomaly detections Anomaly Detection TKDE2022 <p>(iii) Point Clustering :</p> Abbr Algorithm Application Publication IDKC Kernel-based Clustering via Isolation Distributional Kernel. Point Clustering IS2023 PSKC Point-set Kernel Clustering Point Clustering TKDE2023 IKAHC Isolation Kernel for Agglomerative Hierarchical Clustering Hierarchical Clustering PR2023 <p>(IV) Graph Data :</p> Abbr Algorithm Application Publication IKGOD Subgraph Centralization: A Necessary Step for Graph Anomaly Detection. Graph Anomaly Detection SIAM2023 IsoGraphKernel Isolation Graph Kernel Graph IK embedding and similarity calculating AAAI2021 <p>(V) Group Data :</p> Abbr Algorithm Application Publication IKGAD Isolation Distributional Kernel for group anomaly detections Group Anomaly Detection TKDE2022 <p>(VI) Stream Data :</p> Abbr Algorithm Application Publication StreaKHC Isolation Distribution Kernel for Trajectory Anomaly Detections Online Hierarchical Clustering SIGKDD2022 ICID Detecting change intervals with isolation distributional kernel Change Intervals Detection JAIR2024 <p>(VII) Trajectory Data :</p> Abbr Algorithm Application Publication TIDKC Distribution-based Tajectory Clustering Trajectory Clustering ICDM2023 IKAT Isolation Distribution Kernel for Trajectory Anomaly Detections Trajectory Anomaly Detection JAIR2024 <p>(VIII) Time Series</p> Abbr Algorithm Application Publication IKTOD Isolation distribution kernel for Time Series Anomaly Detection Anomaly detection VLDB2022"},{"location":"index.html#features","title":"Features","text":"<p>IKPyKit provides a set of key features designed to make machine learning tasks easy and efficient. For a detailed overview, see the User Guides.</p>"},{"location":"index.html#examples-and-tutorials","title":"Examples and tutorials","text":"<p>Explore our extensive list of examples and tutorials to get you started with IKPyKit. You can find them here.</p>"},{"location":"index.html#how-to-contribute","title":"How to contribute","text":"<p>Primarily, IKPyKit development consists of adding and creating new algorithms, new validation strategies, or improving the performance of the current code. However, there are many other ways to contribute:</p> <ul> <li>Submit a bug report or feature request on GitHub Issues.</li> <li>Contribute a Jupyter notebook to our examples.</li> <li>Write unit or integration tests for our project.</li> <li>Answer questions on our issues, Stack Overflow, and elsewhere.</li> <li>Translate our documentation into another language.</li> <li>Write a blog post, tweet, or share our project with others.</li> </ul> <p>For more information on how to contribute to IKPyKit, see our Contribution Guide.</p> <p>Visit our authors section to meet all the contributors to IKPyKit.</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use IKPyKit for a scientific Publication, we would appreciate citations to the Publication software.</p> <p>BibTeX:</p> <pre><code>@software{IKPyKit,\n    author = {Xin Han, Yixiao Ma, Ye Zhu, and Kaiming Ting},\n    title = {IKPyKit\uff1aA Python Library for Isolation Kernel Toolkit},\n    version = {0.1.0},\n    month = {3},\n    year = {2025},\n    license = {BSD-3-Clause},\n    url = {https://github.com/IsolationKernel/ikpykit}\n}\n</code></pre>"},{"location":"index.html#license","title":"License","text":"<p>BSD-3-Clause License</p>"},{"location":"api/anomaly/idkd.html","title":"IDKD","text":""},{"location":"api/anomaly/idkd.html#ikpykit.anomaly.IDKD","title":"ikpykit.anomaly.IDKD","text":"<pre><code>IDKD(\n    n_estimators=200,\n    max_samples=\"auto\",\n    contamination=\"auto\",\n    method=\"inne\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>Isolation Distributional Kernel for anomaly detection.</p> <p>IDKD measures the similarity between distributions to identify anomalies. An observation is considered anomalous when its Dirac measure has a low similarity with respect to the reference distribution from which the dataset was generated.</p> <p>This implementation follows the algorithm described in [1]_.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <code>int</code> <p>Number of base estimators in the ensemble.</p> <code>200</code> <code>max_samples</code> <code>(auto, int, float)</code> <p>Number of samples to draw from X to train each base estimator.</p> <ul> <li>If \"auto\", then <code>max_samples=min(8, n_samples)</code>.</li> <li>If int, then draw <code>max_samples</code> samples.</li> <li>If float, then draw <code>max_samples * X.shape[0]</code> samples.</li> </ul> <code>\"auto\"</code> <code>method</code> <code>(inne, anne, auto)</code> <p>Isolation method to use. The original algorithm described in [1]_ uses \"inne\".</p> <code>\"inne\"</code> <code>contamination</code> <code>(auto, float)</code> <p>The proportion of outliers in the data set.</p> <ul> <li>If \"auto\", the threshold is determined as in [1]_.</li> <li>If float, the contamination should be in the range (0, 0.5].</li> </ul> <p>Used to define the threshold on the decision function.</p> <code>\"auto\"</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the randomness of the estimator. Pass an int for reproducible results across multiple function calls.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>offset_</code> <code>float</code> <p>Offset used to define the decision function from the raw scores.</p> <code>max_samples_</code> <code>int</code> <p>Actual number of samples used.</p> <code>iso_kernel_</code> <code>IsoKernel</code> <p>The fitted isolation kernel.</p> References <p>.. [1] Kai Ming Ting, Bi-Cun Xu, Washio Takashi, Zhi-Hua Zhou (2022).    \"Isolation Distributional Kernel: A new tool for kernel based point and group anomaly detections.\"    IEEE Transactions on Knowledge and Data Engineering.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.anomaly import IDKD\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[-1.1, 0.2], [0.3, 0.5], [0.5, 1.1], [100, 90]])\n&gt;&gt;&gt; clf = IDKD(max_samples=2, contamination=0.25).fit(X)\n&gt;&gt;&gt; clf.predict([[0.1, 0.3], [0, 0.7], [90, 85]])\narray([ 1,  1, -1])\n</code></pre> Source code in <code>ikpykit/anomaly/_idkd.py</code> <pre><code>def __init__(\n    self,\n    n_estimators=200,\n    max_samples=\"auto\",\n    contamination=\"auto\",\n    method=\"inne\",\n    random_state=None,\n):\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.random_state = random_state\n    self.contamination = contamination\n    self.method = method\n</code></pre>"},{"location":"api/anomaly/idkd.html#ikpykit.anomaly.IDKD.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the IDKD model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Training data. Use <code>dtype=np.float32</code> for maximum efficiency.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>ikpykit/anomaly/_idkd.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the IDKD model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data. Use ``dtype=np.float32`` for maximum efficiency.\n\n    y : Ignored\n        Not used, present for API consistency.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n\n    # Check data\n    X = check_array(X, accept_sparse=False)\n\n    n_samples = X.shape[0]\n    if isinstance(self.max_samples, str):\n        if self.max_samples == \"auto\":\n            max_samples = min(16, n_samples)\n        else:\n            raise ValueError(\n                \"max_samples (%s) is not supported.\"\n                'Valid choices are: \"auto\", int or'\n                \"float\" % self.max_samples\n            )\n\n    elif isinstance(self.max_samples, numbers.Integral):\n        if self.max_samples &gt; n_samples:\n            warn(\n                \"max_samples (%s) is greater than the \"\n                \"total number of samples (%s). max_samples \"\n                \"will be set to n_samples for estimation.\"\n                % (self.max_samples, n_samples)\n            )\n            max_samples = n_samples\n        else:\n            max_samples = self.max_samples\n    else:  # float\n        if not 0.0 &lt; self.max_samples &lt;= 1.0:\n            raise ValueError(\n                \"max_samples must be in (0, 1], got %r\" % self.max_samples\n            )\n        max_samples = int(self.max_samples * X.shape[0])\n\n    self.max_samples_ = max_samples\n    self._fit(X)\n    self.is_fitted_ = True\n\n    if self.contamination != \"auto\":\n        if not (0.0 &lt; self.contamination &lt;= 0.5):\n            raise ValueError(\n                \"contamination must be in (0, 0.5], got: %f\" % self.contamination\n            )\n\n    if self.contamination == \"auto\":\n        # 0.5 plays a special role as described in the original paper.\n        # we take the opposite as we consider the opposite of their score.\n        self.offset_ = -0.5\n    else:\n        # else, define offset_ wrt contamination parameter\n        self.offset_ = np.percentile(\n            self.score_samples(X), 100.0 * self.contamination\n        )\n\n    return self\n</code></pre>"},{"location":"api/anomaly/idkd.html#ikpykit.anomaly.IDKD.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict if samples are outliers or not.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The query samples.</p> required <p>Returns:</p> Name Type Description <code>is_inlier</code> <code>ndarray of shape (n_samples,)</code> <p>Returns +1 for inliers and -1 for outliers.</p> Source code in <code>ikpykit/anomaly/_idkd.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict if samples are outliers or not.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The query samples.\n\n    Returns\n    -------\n    is_inlier : ndarray of shape (n_samples,)\n        Returns +1 for inliers and -1 for outliers.\n    \"\"\"\n    check_is_fitted(self)\n    decision_func = self.decision_function(X)\n    is_inlier = np.ones_like(decision_func, dtype=int)\n    is_inlier[decision_func &lt; 0] = -1\n    return is_inlier\n</code></pre>"},{"location":"api/anomaly/idkd.html#ikpykit.anomaly.IDKD.decision_function","title":"decision_function","text":"<pre><code>decision_function(X)\n</code></pre> <p>Compute the decision function for each sample.</p> <p>The decision function is defined as score_samples(X) - offset_. Negative values are considered outliers and positive values are considered inliers.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The query samples.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>Decision function values for each sample. Negative values represent outliers, positive values represent inliers.</p> Source code in <code>ikpykit/anomaly/_idkd.py</code> <pre><code>def decision_function(self, X):\n    \"\"\"Compute the decision function for each sample.\n\n    The decision function is defined as score_samples(X) - offset_.\n    Negative values are considered outliers and positive values are considered inliers.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The query samples.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        Decision function values for each sample.\n        Negative values represent outliers, positive values represent inliers.\n    \"\"\"\n    # We subtract self.offset_ to make 0 be the threshold value for being\n    # an outlier.\n    return self.score_samples(X) - self.offset_\n</code></pre>"},{"location":"api/anomaly/idkd.html#ikpykit.anomaly.IDKD.score_samples","title":"score_samples","text":"<pre><code>score_samples(X)\n</code></pre> <p>Compute the anomaly scores for each sample.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The query samples.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>The anomaly score of each input sample. The lower the score, the more anomalous the sample.</p> Source code in <code>ikpykit/anomaly/_idkd.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"Compute the anomaly scores for each sample.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The query samples.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        The anomaly score of each input sample.\n        The lower the score, the more anomalous the sample.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    # Check data\n    X = check_array(X, accept_sparse=False)\n\n    X_trans = self.iso_kernel_.transform(X)\n    kme = np.average(X_trans.toarray(), axis=0) / self.max_samples_\n    scores = safe_sparse_dot(X_trans, kme.T).flatten()\n\n    return scores\n</code></pre>"},{"location":"api/anomaly/iforest.html","title":"IForest","text":""},{"location":"api/anomaly/iforest.html#ikpykit.anomaly.IForest","title":"ikpykit.anomaly.IForest","text":"<pre><code>IForest(\n    n_estimators=100,\n    max_samples=\"auto\",\n    contamination=0.1,\n    max_features=1.0,\n    bootstrap=False,\n    n_jobs=1,\n    random_state=None,\n    verbose=0,\n)\n</code></pre> <p>               Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>Wrapper of scikit-learn Isolation Forest for anomaly detection.</p> <p>The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</p> <p>Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.</p> <p>This path length, averaged over a forest of such random trees, is a measure of normality and our decision function. Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <code>int</code> <p>The number of base estimators (trees) in the ensemble.</p> <code>100</code> <code>max_samples</code> <code>int or float</code> <p>The number of samples to draw from X to train each base estimator. - If int, then draw <code>max_samples</code> samples. - If float, then draw <code>max_samples * X.shape[0]</code> samples. - If \"auto\", then <code>max_samples=min(256, n_samples)</code>.</p> <code>\"auto\"</code> <code>contamination</code> <code>float or auto</code> <p>The proportion of outliers in the data set. Used to define the threshold on the scores of the samples. - If 'auto', the threshold is determined as in the original paper. - If float, the contamination should be in the range (0, 0.5].</p> <code>0.1</code> <code>max_features</code> <code>int or float</code> <p>The number of features to draw from X to train each base estimator. - If int, then draw <code>max_features</code> features. - If float, then draw <code>max_features * X.shape[1]</code> features.</p> <code>1.0</code> <code>bootstrap</code> <code>bool</code> <p>If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>. If -1, then the number of jobs is set to the number of cores.</p> <code>1</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest. Pass an int for reproducible results across multiple function calls.</p> <code>None</code> <code>verbose</code> <code>int</code> <p>Controls the verbosity of the tree building process.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>detector_</code> <code>IsolationForest</code> <p>The underlying scikit-learn IsolationForest object.</p> <code>is_fitted_</code> <code>bool</code> <p>Indicates whether the estimator has been fitted.</p> References <p>.. [1] Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2008, December). \"Isolation forest.\"        In 2008 Eighth IEEE International Conference on Data Mining (pp. 413-422). IEEE.</p> <p>.. [2] Liu, F. T., Ting, K. M., &amp; Zhou, Z. H. (2012). \"Isolation-based        anomaly detection.\" ACM Transactions on Knowledge Discovery from        Data (TKDD), 6(1), 1-39.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.anomaly import IForest\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[-1.1, 0.2], [0.3, 0.5], [0.5, 1.1], [100, 90]])\n&gt;&gt;&gt; clf = IForest(contamination=0.25).fit(X)\n&gt;&gt;&gt; clf.predict([[0.1, 0.3], [0, 0.7], [90, 85]])\narray([ 1,  1, -1])\n</code></pre> Source code in <code>ikpykit/anomaly/_iforest.py</code> <pre><code>def __init__(\n    self,\n    n_estimators=100,\n    max_samples=\"auto\",\n    contamination=0.1,\n    max_features=1.0,\n    bootstrap=False,\n    n_jobs=1,\n    random_state=None,\n    verbose=0,\n):\n    self.contamination = contamination\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.max_features = max_features\n    self.bootstrap = bootstrap\n    self.n_jobs = n_jobs\n    self.random_state = random_state\n    self.verbose = verbose\n</code></pre>"},{"location":"api/anomaly/iforest.html#ikpykit.anomaly.IForest.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the isolation forest model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples. Use <code>dtype=np.float32</code> for maximum efficiency.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>ikpykit/anomaly/_iforest.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    Fit the isolation forest model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Use ``dtype=np.float32`` for maximum\n        efficiency.\n\n    y : Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n    # Check data\n    X = check_array(X, accept_sparse=False)\n\n    self.detector_ = IsolationForest(\n        n_estimators=self.n_estimators,\n        max_samples=self.max_samples,\n        contamination=self.contamination,\n        max_features=self.max_features,\n        bootstrap=self.bootstrap,\n        n_jobs=self.n_jobs,\n        random_state=self.random_state,\n        verbose=self.verbose,\n    )\n\n    self.detector_.fit(X=X, y=None, sample_weight=None)\n    self.is_fitted_ = True\n\n    return self\n</code></pre>"},{"location":"api/anomaly/iforest.html#ikpykit.anomaly.IForest.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict if a particular sample is an outlier or not.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples.</p> required <p>Returns:</p> Name Type Description <code>is_inlier</code> <code>ndarray of shape (n_samples,)</code> <p>The predicted labels. +1 for inliers, -1 for outliers.</p> Source code in <code>ikpykit/anomaly/_iforest.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict if a particular sample is an outlier or not.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    is_inlier : ndarray of shape (n_samples,)\n        The predicted labels. +1 for inliers, -1 for outliers.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    return self.detector_.predict(X)\n</code></pre>"},{"location":"api/anomaly/iforest.html#ikpykit.anomaly.IForest.decision_function","title":"decision_function","text":"<pre><code>decision_function(X)\n</code></pre> <p>Compute the anomaly score for each sample.</p> <p>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</p> Source code in <code>ikpykit/anomaly/_iforest.py</code> <pre><code>def decision_function(self, X):\n    \"\"\"\n    Compute the anomaly score for each sample.\n\n    The anomaly score of an input sample is computed as\n    the mean anomaly score of the trees in the forest.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        The anomaly score of the input samples.\n        The lower, the more abnormal. Negative scores represent outliers,\n        positive scores represent inliers.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    return self.detector_.decision_function(X)\n</code></pre>"},{"location":"api/anomaly/iforest.html#ikpykit.anomaly.IForest.score_samples","title":"score_samples","text":"<pre><code>score_samples(X)\n</code></pre> <p>Return the raw anomaly score of samples.</p> <p>The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>The raw anomaly score of the input samples. The lower, the more abnormal.</p> Source code in <code>ikpykit/anomaly/_iforest.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"\n    Return the raw anomaly score of samples.\n\n    The anomaly score of an input sample is computed as\n    the mean anomaly score of the trees in the forest.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        The raw anomaly score of the input samples.\n        The lower, the more abnormal.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    # Check data\n    X = check_array(X, accept_sparse=False)\n    return self.detector_.score_samples(X)\n</code></pre>"},{"location":"api/anomaly/inne.html","title":"INNE","text":""},{"location":"api/anomaly/inne.html#ikpykit.anomaly.INNE","title":"ikpykit.anomaly.INNE","text":"<pre><code>INNE(\n    n_estimators=200,\n    max_samples=\"auto\",\n    contamination=\"auto\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>Isolation-based anomaly detection using nearest-neighbor ensembles.</p> <p>The INNE algorithm uses the nearest neighbour ensemble to isolate anomalies. It partitions the data space into regions using a subsample and determines an isolation score for each region. As each region adapts to local distribution, the calculated isolation score is a local measure that is relative to the local neighbourhood, enabling it to detect both global and local anomalies. INNE has linear time complexity to efficiently handle large and high-dimensional datasets with complex distributions.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the ensemble.</p> <code>200</code> <code>max_samples</code> <code>int</code> <p>The number of samples to draw from X to train each base estimator.</p> <pre><code>- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples` * X.shape[0]` samples.\n- If \"auto\", then `max_samples=min(8, n_samples)`.\n</code></pre> <code>\"auto\"</code> <code>contamination</code> <code>auto or float</code> <p>The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.</p> <pre><code>- If \"auto\", the threshold is determined as in the original paper.\n- If float, the contamination should be in the range (0, 0.5].\n</code></pre> <code>\"auto\"</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.</p> <p>Pass an int for reproducible results across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> References <p>.. [1] T. R. Bandaragoda, K. Ming Ting, D. Albrecht, F. T. Liu, Y. Zhu, and J. R. Wells.        \"Isolation-based anomaly detection using nearest-neighbor ensembles.\" In Computational        Intelligence, vol. 34, 2018, pp. 968-998.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.anomaly import INNE\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[-1.1, 0.2], [0.3, 0.5], [0.5, 1.1], [100, 90]])\n&gt;&gt;&gt; clf = INNE(contamination=0.25).fit(X)\n&gt;&gt;&gt; clf.predict([[0.1, 0.3], [0, 0.7], [90, 85]])\narray([ 1,  1, -1])\n</code></pre> Source code in <code>ikpykit/anomaly/_inne.py</code> <pre><code>def __init__(\n    self,\n    n_estimators=200,\n    max_samples=\"auto\",\n    contamination=\"auto\",\n    random_state=None,\n):\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.random_state = random_state\n    self.contamination = contamination\n</code></pre>"},{"location":"api/anomaly/inne.html#ikpykit.anomaly.INNE.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit estimator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples. Use <code>dtype=np.float32</code> for maximum efficiency.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>ikpykit/anomaly/_inne.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    Fit estimator.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Use ``dtype=np.float32`` for maximum\n        efficiency.\n\n    y : Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n\n    # Check data\n    X = check_array(X, accept_sparse=False)\n\n    n_samples = X.shape[0]\n    if isinstance(self.max_samples, str):\n        if self.max_samples == \"auto\":\n            max_samples = min(16, n_samples)\n        else:\n            raise ValueError(\n                \"max_samples (%s) is not supported.\"\n                'Valid choices are: \"auto\", int or'\n                \"float\" % self.max_samples\n            )\n\n    elif isinstance(self.max_samples, numbers.Integral):\n        if self.max_samples &gt; n_samples:\n            warn(\n                \"max_samples (%s) is greater than the \"\n                \"total number of samples (%s). max_samples \"\n                \"will be set to n_samples for estimation.\"\n                % (self.max_samples, n_samples)\n            )\n            max_samples = n_samples\n        else:\n            max_samples = self.max_samples\n    else:  # float\n        if not 0.0 &lt; self.max_samples &lt;= 1.0:\n            raise ValueError(\n                \"max_samples must be in (0, 1], got %r\" % self.max_samples\n            )\n        max_samples = int(self.max_samples * X.shape[0])\n\n    self.max_samples_ = max_samples\n    self._fit(X)\n    self.is_fitted_ = True\n\n    if self.contamination != \"auto\":\n        if not (0.0 &lt; self.contamination &lt;= 0.5):\n            raise ValueError(\n                \"contamination must be in (0, 0.5], got: %f\" % self.contamination\n            )\n\n    if self.contamination == \"auto\":\n        # 0.5 plays a special role as described in the original paper.\n        # we take the opposite as we consider the opposite of their score.\n        self.offset_ = -0.5\n    else:\n        # else, define offset_ wrt contamination parameter\n        self.offset_ = np.percentile(\n            self.score_samples(X), 100.0 * self.contamination\n        )\n\n    return self\n</code></pre>"},{"location":"api/anomaly/inne.html#ikpykit.anomaly.INNE.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict if a particular sample is an outlier or not.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples. Internally, it will be converted to <code>dtype=np.float32</code> and if a sparse matrix is provided to a sparse <code>csr_matrix</code>.</p> required <p>Returns:</p> Name Type Description <code>is_inlier</code> <code>ndarray of shape (n_samples,)</code> <p>For each observation, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.</p> Source code in <code>ikpykit/anomaly/_inne.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict if a particular sample is an outlier or not.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n\n    Returns\n    -------\n    is_inlier : ndarray of shape (n_samples,)\n        For each observation, tells whether or not (+1 or -1) it should\n        be considered as an inlier according to the fitted model.\n    \"\"\"\n\n    check_is_fitted(self)\n    decision_func = self.decision_function(X)\n    is_inlier = np.ones_like(decision_func, dtype=int)\n    is_inlier[decision_func &lt; 0] = -1\n    return is_inlier\n</code></pre>"},{"location":"api/anomaly/inne.html#ikpykit.anomaly.INNE.decision_function","title":"decision_function","text":"<pre><code>decision_function(X)\n</code></pre> <p>Average anomaly score of X of the base classifiers.</p> <p>The anomaly score of an input sample is computed as the mean anomaly score of the .</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples. Internally, it will be converted to <code>dtype=np.float32</code>.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>The anomaly score of the input samples. The lower, the more abnormal. Negative scores represent outliers, positive scores represent inliers.</p> Source code in <code>ikpykit/anomaly/_inne.py</code> <pre><code>def decision_function(self, X):\n    \"\"\"\n    Average anomaly score of X of the base classifiers.\n\n    The anomaly score of an input sample is computed as\n    the mean anomaly score of the .\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32``.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        The anomaly score of the input samples.\n        The lower, the more abnormal. Negative scores represent outliers,\n        positive scores represent inliers.\n    \"\"\"\n    # We subtract self.offset_ to make 0 be the threshold value for being\n    # an outlier.\n\n    return self.score_samples(X) - self.offset_\n</code></pre>"},{"location":"api/anomaly/inne.html#ikpykit.anomaly.INNE.score_samples","title":"score_samples","text":"<pre><code>score_samples(X)\n</code></pre> <p>Opposite of the anomaly score defined in the original paper. The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>The anomaly score of the input samples. The lower, the more abnormal.</p> Source code in <code>ikpykit/anomaly/_inne.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"\n    Opposite of the anomaly score defined in the original paper.\n    The anomaly score of an input sample is computed as\n    the mean anomaly score of the trees in the forest.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        The anomaly score of the input samples.\n        The lower, the more abnormal.\n    \"\"\"\n\n    check_is_fitted(self, \"is_fitted_\")\n    # Check data\n    X = check_array(X, accept_sparse=False)\n\n    isolation_scores = np.ones([self.n_estimators, X.shape[0]])\n    # each test instance is evaluated against n_estimators sets of hyperspheres\n    for i in range(self.n_estimators):\n        x_dists = euclidean_distances(X, self._centroids[i], squared=True)\n        # find instances that are covered by at least one hypersphere.\n        cover_radius = np.where(\n            x_dists &lt;= self._centroids_radius[i], self._centroids_radius[i], np.nan\n        )\n        x_covered = np.where(~np.isnan(cover_radius).all(axis=1))\n        # the centroid of the hypersphere covering x and having the smallest radius\n        cnn_x = np.nanargmin(cover_radius[x_covered], axis=1)\n        isolation_scores[i][x_covered] = self._ratio[i][cnn_x]\n    # the isolation scores are averaged to produce the anomaly score\n    scores = np.mean(isolation_scores, axis=0)\n    return -scores\n</code></pre>"},{"location":"api/cluster/idkc.html","title":"IDKC","text":""},{"location":"api/cluster/idkc.html#ikpykit.cluster.IDKC","title":"ikpykit.cluster.IDKC","text":"<pre><code>IDKC(\n    n_estimators,\n    max_samples,\n    method,\n    k,\n    kn,\n    v,\n    n_init_samples,\n    init_center=None,\n    is_post_process=True,\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>Isolation Distributional Kernel Clustering.</p> <p>A clustering algorithm that leverages Isolation Kernels to transform data into a feature space where cluster structures are more distinguishable. The algorithm first constructs Isolation Kernel representations, then performs clustering in this transformed space using a threshold-based assignment mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <code>int</code> <p>Number of base estimators in the ensemble for the Isolation Kernel. Higher values generally lead to more stable results but increase computation time.</p> required <code>max_samples</code> <code>int</code> <p>Number of samples to draw from X to train each base estimator in the Isolation Kernel. Controls the granularity of the kernel representation.</p> required <code>method</code> <code>(inne, anne, iforest)</code> <p>Method used to calculate the Isolation Kernel: - 'inne': Isolation Nearest Neighbor Ensemble - 'anne': Approximate Nearest Neighbor Ensemble - 'iforest': Isolation Forest</p> <code>'inne'</code> <code>k</code> <code>int</code> <p>Number of clusters to form in the dataset.</p> required <code>kn</code> <code>int</code> <p>Number of nearest neighbors used for local contrast density calculation during initialization. Higher values consider more neighbors when determining density.</p> required <code>v</code> <code>float</code> <p>Decay factor (0 &lt; v &lt; 1) for reducing the similarity threshold during clustering. Smaller values cause faster decay, leading to more aggressive cluster assignments.</p> required <code>n_init_samples</code> <code>int or float</code> <p>If int, number of samples to consider when initializing cluster centers. If float, fraction of total samples to consider when initializing cluster centers. Number of samples to consider when initializing cluster centers. Larger values may produce better initial centers but increase computation.</p> required <code>init_center</code> <code>int or array-like of shape (k,)</code> <p>Index or indices of initial cluster centers. If None, centers are selected automatically based on density and distance considerations.</p> <code>None</code> <code>is_post_process</code> <code>bool</code> <p>Whether to perform post-processing refinement of clusters through iterative reassignment. Improves cluster quality but adds computational overhead.</p> <code>True</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the randomness of the algorithm. Pass an int for reproducible results.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>clusters_</code> <code>list of KCluster objects</code> <p>The cluster objects containing assignment and centroid information.</p> <code>it_</code> <code>int</code> <p>Number of iterations performed during the initial clustering phase.</p> <code>labels_</code> <code>ndarray of shape (n_samples,)</code> <p>Cluster labels for each point. Points not assigned to any cluster have label -1 (outliers).</p> <code>is_fitted_</code> <code>bool</code> <p>Whether the model has been fitted to data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.cluster import IDKC\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [5, 2], [5, 5],  [1, 0], [5, 0]])\n&gt;&gt;&gt; clustering = IDKC(\n...     n_estimators=100, max_samples=3, method='anne',\n...     k=2, kn=5, v=0.5, n_init_samples=4, random_state=42\n... )\n&gt;&gt;&gt; clustering.fit_predict(X)\narray([1, 1, 0, 0, 1, 0])\n</code></pre> References <p>.. [1] Ye Zhu, Kai Ming Ting (2023). Kernel-based Clustering via Isolation Distributional Kernel. Information Systems.</p> Source code in <code>ikpykit/cluster/_idkc.py</code> <pre><code>def __init__(\n    self,\n    n_estimators,\n    max_samples,\n    method,\n    k,\n    kn,\n    v,\n    n_init_samples,\n    init_center=None,\n    is_post_process=True,\n    random_state=None,\n):\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.method = method\n    self.k = k\n    self.kn = kn\n    self.v = v\n    self.n_init_samples = n_init_samples\n    self.is_post_process = is_post_process\n    self.init_center = init_center\n    self.random_state = random_state\n    self.clusters_ = []\n    self.it_ = 0\n    self.labels_ = None\n    self.data_index = None\n</code></pre>"},{"location":"api/cluster/idkc.html#ikpykit.cluster.IDKC.n_it","title":"n_it  <code>property</code>","text":"<pre><code>n_it\n</code></pre> <p>Get number of iterations performed during clustering.</p>"},{"location":"api/cluster/idkc.html#ikpykit.cluster.IDKC.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the IDKC clustering model on data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_features)</code> <p>The input instances to cluster.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>ikpykit/cluster/_idkc.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the IDKC clustering model on data X.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        The input instances to cluster.\n    y : Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n    X = check_array(X)\n    if self.n_init_samples &lt;= 0:\n        raise ValueError(\n            f\"Number of initial samples n_init_samples={self.n_init_samples} must be greater than 0\"\n        )\n    elif isinstance(self.n_init_samples, numbers.Integral):\n        if self.n_init_samples &gt; X.shape[0]:\n            self.n_init_samples = X.shape[0]\n            raise warn(\n                f\"Number of initial samples n_init_samples={self.n_init_samples} is greater than the number of samples in the dataset. Setting n_init_samples to {X.shape[0]}\"\n            )\n        else:\n            self.n_init_samples = int(self.n_init_samples)\n    elif isinstance(self.n_init_samples, float):\n        if not (0 &lt; self.n_init_samples &lt;= 1):\n            raise ValueError(\n                f\"Fraction of initial samples n_init_samples={self.n_init_samples} must be between 0 and 1\"\n            )\n        self.n_init_samples = int(self.n_init_samples * X.shape[0])\n    self.data_index = np.arange(X.shape[0])\n    isokernel = IsoKernel(\n        method=self.method,\n        max_samples=self.max_samples,\n        n_estimators=self.n_estimators,\n        random_state=self.random_state,\n    )\n    data_ik = isokernel.fit_transform(X)\n    self._fit(data_ik)\n\n    # Apply post-processing if requested\n    if self.is_post_process:\n        self._post_process(data_ik)\n\n    self.is_fitted_ = True\n    self.labels_ = self._get_labels(X)\n    return self\n</code></pre>"},{"location":"api/cluster/idkc.html#ikpykit.cluster.IDKC.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict the cluster labels for each point in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of shape (n_samples, n_features)</code> <p>The input instances to predict cluster labels for.</p> required <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray of shape (n_samples,)</code> <p>Cluster labels for each point. Points not assigned to any cluster have label -1 (outliers).</p> Source code in <code>ikpykit/cluster/_idkc.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict the cluster labels for each point in X.\n\n    Parameters\n    ----------\n    X : ndarray of shape (n_samples, n_features)\n        The input instances to predict cluster labels for.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point. Points not assigned to any cluster\n        have label -1 (outliers).\n    \"\"\"\n    X = check_array(X)\n\n    return self._get_labels\n</code></pre>"},{"location":"api/cluster/ikahc.html","title":"IKAHC","text":""},{"location":"api/cluster/ikahc.html#ikpykit.cluster.IKAHC","title":"ikpykit.cluster.IKAHC","text":"<pre><code>IKAHC(\n    n_estimators=200,\n    max_samples=\"auto\",\n    lk_method=\"single\",\n    ik_method=\"anne\",\n    return_flat=False,\n    t=None,\n    n_clusters=None,\n    criterion=\"distance\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>IKAHC is a novel hierarchical clustering algorithm. It uses a data-dependent kernel called Isolation Kernel to measure the similarity between clusters.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the ensemble.</p> <code>200</code> <code>max_samples</code> <code>int or float or str</code> <p>The number of samples to draw from X to train each base estimator.</p> <pre><code>- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples * X.shape[0]` samples.\n- If \"auto\", then `max_samples=min(8, n_samples)`.\n</code></pre> <code>\"auto\"</code> <code>ik_method</code> <code>Literal['inne', 'anne']</code> <p>Isolation method to use. The original algorithm in paper is <code>\"anne\"</code>.</p> <code>'anne'</code> <code>lk_method</code> <code>(single, complete, average, weighted)</code> <p>The linkage algorithm to use. The supported Linkage Methods are 'single', 'complete', 'average' and 'weighted'.</p> <code>\"single\"</code> <code>return_flat</code> <code>bool</code> <p>Whether to return flat clusters that extract from the fitted dendrogram.</p> <code>False</code> <code>t</code> <code>float</code> <p>The threshold to apply when forming flat clusters. Either t or n_clusters should be provided.</p> <code>None</code> <code>n_clusters</code> <code>int</code> <p>The number of flat clusters to form. Either t or n_clusters should be provided.</p> <code>None</code> <code>criterion</code> <code>str</code> <p>The criterion to use in forming flat clusters. Valid options are 'distance', 'inconsistent', 'maxclust', or 'monocrit'.</p> <code>'distance'</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the pseudo-randomness of the selection of the samples to fit the Isolation Kernel.</p> <p>Pass an int for reproducible results across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>isokernel</code> <code>IsoKernel</code> <p>Fitted isolation kernel.</p> <code>dendrogram</code> <code>ndarray</code> <p>Cluster hierarchy as computed by scipy.cluster.hierarchy.linkage.</p> References <p>.. [1] Xin Han, Ye Zhu, Kai Ming Ting, and Gang Li,        \"The Impact of Isolation Kernel on Agglomerative Hierarchical Clustering Algorithms\",        Pattern Recognition, 2023, 139: 109517.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.cluster import IKAHC\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = [[0.4,0.3], [0.3,0.8], [0.5, 0.4], [0.5, 0.1]]\n&gt;&gt;&gt; clf = IKAHC(n_estimators=200, max_samples=2, lk_method='single', n_clusters=2, return_flat=True)\n&gt;&gt;&gt; clf.fit_predict(X)\narray([1, 2, 1, 1], dtype=int32)\n</code></pre> Source code in <code>ikpykit/cluster/_ikahc.py</code> <pre><code>def __init__(\n    self,\n    n_estimators: int = 200,\n    max_samples: Union[int, float, str] = \"auto\",\n    lk_method: Literal[\"single\", \"complete\", \"average\", \"weighted\"] = \"single\",\n    ik_method: Literal[\"inne\", \"anne\"] = \"anne\",\n    return_flat: bool = False,\n    t: Optional[float] = None,\n    n_clusters: Optional[int] = None,\n    criterion: str = \"distance\",\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n):\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.ik_method = ik_method\n    self.lk_method = lk_method\n    self.return_flat = return_flat\n    self.t = t\n    self.n_clusters = n_clusters\n    self.criterion = criterion\n    self.random_state = random_state\n    self.labels_ = None\n</code></pre>"},{"location":"api/cluster/ikahc.html#ikpykit.cluster.IKAHC.dendrogram","title":"dendrogram  <code>property</code>","text":"<pre><code>dendrogram\n</code></pre> <p>Get the dendrogram of the hierarchical clustering.</p> <p>Returns:</p> Name Type Description <code>dendrogram_</code> <code>ndarray</code> <p>The dendrogram representing the hierarchical clustering.</p>"},{"location":"api/cluster/ikahc.html#ikpykit.cluster.IKAHC.isokernel","title":"isokernel  <code>property</code>","text":"<pre><code>isokernel\n</code></pre> <p>Get the fitted isolation kernel.</p> <p>Returns:</p> Name Type Description <code>isokernel_</code> <code>IsoKernel</code> <p>The fitted isolation kernel.</p>"},{"location":"api/cluster/ikahc.html#ikpykit.cluster.IKAHC.fit","title":"fit","text":"<pre><code>fit(X)\n</code></pre> <p>Fit the IKAHC clustering model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>ikpykit/cluster/_ikahc.py</code> <pre><code>def fit(self, X: np.ndarray) -&gt; \"IKAHC\":\n    \"\"\"Fit the IKAHC clustering model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n    # Check data\n    X = check_array(X, accept_sparse=False)\n\n    # Validate parameters\n    if self.lk_method not in [\"single\", \"complete\", \"average\", \"weighted\"]:\n        raise ValueError(\n            f\"lk_method must be one of 'single', 'complete', 'average', 'weighted', got {self.lk_method}\"\n        )\n\n    if self.ik_method not in [\"inne\", \"anne\"]:\n        raise ValueError(\n            f\"ik_method must be one of 'inne', 'anne', got {self.ik_method}\"\n        )\n\n    if self.n_estimators &lt;= 0:\n        raise ValueError(f\"n_estimators must be positive, got {self.n_estimators}\")\n\n    # Check if both t and n_clusters are provided at initialization\n    if self.return_flat and self.t is not None and self.n_clusters is not None:\n        raise ValueError(\n            \"Specify either a distance threshold t or n_clusters, not both.\"\n        )\n\n    # Fit isolation kernel\n    self.isokernel_ = IsoKernel(\n        method=self.ik_method,\n        n_estimators=self.n_estimators,\n        max_samples=self.max_samples,\n        random_state=self.random_state,\n    )\n    self.isokernel_ = self.isokernel_.fit(X)\n\n    # Calculate similarity matrix and convert to distance matrix (1-similarity)\n    similarity_matrix = self.isokernel_.similarity(X)\n    self.dendrogram_ = linkage(1 - similarity_matrix, method=self.lk_method)\n\n    if self.return_flat:\n        self.labels_ = self._extract_flat_cluster()\n\n    return self\n</code></pre>"},{"location":"api/cluster/ikahc.html#ikpykit.cluster.IKAHC.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(X, y=None)\n</code></pre> <p>Fit algorithm to data and return the dendrogram.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency by convention.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dendrogram</code> <code>ndarray</code> <p>Dendrogram representing the hierarchical clustering.</p> Source code in <code>ikpykit/cluster/_ikahc.py</code> <pre><code>def fit_transform(self, X: np.ndarray, y: Any = None) -&gt; np.ndarray:\n    \"\"\"Fit algorithm to data and return the dendrogram.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples.\n\n    y : Ignored\n        Not used, present for API consistency by convention.\n\n    Returns\n    -------\n    dendrogram : np.ndarray\n        Dendrogram representing the hierarchical clustering.\n    \"\"\"\n    self.fit(X)\n    return self.dendrogram\n</code></pre>"},{"location":"api/cluster/ikahc.html#ikpykit.cluster.IKAHC.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X, y=None)\n</code></pre> <p>Fit algorithm to data and return the cluster labels.</p> Source code in <code>ikpykit/cluster/_ikahc.py</code> <pre><code>def fit_predict(self, X, y=None):\n    \"\"\"Fit algorithm to data and return the cluster labels.\"\"\"\n    return super().fit_predict(X, y)\n</code></pre>"},{"location":"api/cluster/pskc.html","title":"PSKC","text":""},{"location":"api/cluster/pskc.html#ikpykit.cluster.PSKC","title":"ikpykit.cluster.PSKC","text":"<pre><code>PSKC(\n    n_estimators=200,\n    max_samples=\"auto\",\n    method=\"inne\",\n    tau=0.1,\n    v=0.1,\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>Point-Set Kernel Clustering algorithm using Isolation Kernels.</p> <p>PSKC is a clustering algorithm that leverages Isolation Kernels to create feature vector representations of data points. It adaptively captures the characteristics of local data distributions by using data-dependent kernels. The algorithm forms clusters by identifying points with high similarity in the transformed kernel space.</p> <p>The clustering process works by iteratively: 1. Selecting a center point with maximum similarity to the mean 2. Forming a cluster around this center 3. Removing these points from consideration 4. Continuing until stopping criteria are met</p> <p>n_estimators : int, default=200     The number of base estimators (trees) in the isolation ensemble.</p> <p>max_samples : int or str, default=\"auto\"     - If int, then draw <code>max_samples</code> samples.     - If \"auto\", then <code>max_samples=min(256, n_samples)</code>.</p> <p>method : {'inne', 'anne'}, default='inne'     The method used for building the isolation kernel.</p> <p>tau : float, default=0.1     Lower values result in more clusters.</p> <p>v : float, default=0.1     The decay factor for reducing the similarity threshold.     Controls the expansion of clusters.</p> <pre><code>Controls the pseudo-randomness of the algorithm for reproducibility.\nPass an int for reproducible results across multiple function calls.\n</code></pre> <p>Attributes clusters_ : list     List of KCluster objects representing the identified clusters.</p> <p>labels_ : ndarray of shape (n_samples,)     Cluster labels for each point in the dataset.</p> <p>centers : list     Centers of each cluster in the transformed feature space.</p> <p>n_classes : int     Number of clusters found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.cluster import PSKC\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [10, 2], [10, 10],  [1, 0], [1, 1]])\n&gt;&gt;&gt; pskc = PSKC(n_estimators=100, max_samples=2, tau=0.3, v=0.1, random_state=24)\n&gt;&gt;&gt; pskc.fit_predict(X)\narray([0, 0, 1, 1, 0, 0])\n</code></pre> References <p>.. [1] Kai Ming Ting, Jonathan R. Wells, Ye Zhu (2023) \"Point-set Kernel Clustering\". IEEE Transactions on Knowledge and Data Engineering. Vol.35, 5147-5158.</p> Source code in <code>ikpykit/cluster/_pskc.py</code> <pre><code>def __init__(\n    self,\n    n_estimators=200,\n    max_samples=\"auto\",\n    method=\"inne\",\n    tau=0.1,\n    v=0.1,\n    random_state=None,\n):\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.method = method\n    self.tau = tau\n    self.v = v\n    self.random_state = random_state\n    self.clusters_ = []\n    self.labels_ = None\n</code></pre>"},{"location":"api/cluster/pskc.html#ikpykit.cluster.PSKC.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the model on data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array of shape (n_samples, n_features)</code> <p>The input instances.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> Source code in <code>ikpykit/cluster/_pskc.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the model on data X.\n    Parameters\n    ----------\n    X : np.array of shape (n_samples, n_features)\n        The input instances.\n    Returns\n    -------\n    self : object\n    \"\"\"\n    X = check_array(X)\n    isokernel = IsoKernel(\n        max_samples=self.max_samples,\n        n_estimators=self.n_estimators,\n        random_state=self.random_state,\n        method=self.method,\n    )\n    ndata = isokernel.fit_transform(X)\n    self._fit(ndata)\n    self.is_fitted_ = True\n    self.labels_ = self._get_labels(X)\n    return self\n</code></pre>"},{"location":"api/graph/IsoGraphKernel.html","title":"IsoGraphKernel","text":""},{"location":"api/graph/IsoGraphKernel.html#ikpykit.graph.IsoGraphKernel","title":"ikpykit.graph.IsoGraphKernel","text":"<pre><code>IsoGraphKernel(\n    method=\"anne\",\n    n_estimators=200,\n    max_samples=\"auto\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code></p> <p>Isolation Graph Kernel is a new way to measure the similarity between two graphs.</p> <p>It addresses two key issues of kernel mean embedding, where the kernel employed has: (i) a feature map with intractable dimensionality which leads to high computational cost; and (ii) data independency which leads to poor detection accuracy in anomaly detection.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to compute the isolation kernel feature. The available methods are: <code>anne</code>, <code>inne</code>, and <code>iforest</code>.</p> <code>\"anne\"</code> <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the ensemble.</p> <code>200</code> <code>max_samples</code> <code>int or float or str</code> <p>The number of samples to draw from X to train each base estimator.</p> <pre><code>- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples` * X.shape[0]` samples.\n- If \"auto\", then `max_samples=min(8, n_samples)`.\n</code></pre> <code>\"auto\"</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.</p> <p>Pass an int for reproducible results across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> References <p>.. [1] Bi-Cun Xu, Kai Ming Ting and Yuan Jiang. 2021. \"Isolation Graph Kernel\". In Proceedings of The Thirty-Fifth AAAI Conference on Artificial Intelligence. 10487-10495.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.graph import IsoGraphKernel\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[0.4, 0.3], [0.3, 0.8], [0.5, 0.4], [0.5, 0.1]])\n&gt;&gt;&gt; adjacency = np.array([[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]])\n&gt;&gt;&gt; igk = IsoGraphKernel()\n&gt;&gt;&gt; igk = igk.fit(X)\n&gt;&gt;&gt; embedding = igk.transform(adjacency, X, h=2)\n</code></pre> Source code in <code>ikpykit/graph/_isographkernel.py</code> <pre><code>def __init__(\n    self,\n    method: str = \"anne\",\n    n_estimators: int = 200,\n    max_samples: Union[int, float, str] = \"auto\",\n    random_state: Optional[int] = None,\n) -&gt; None:\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.random_state = random_state\n    self.method = method\n</code></pre>"},{"location":"api/graph/IsoGraphKernel.html#ikpykit.graph.IsoGraphKernel.fit","title":"fit","text":"<pre><code>fit(features)\n</code></pre> <p>Fit the model on data X.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>(csr_matrix, ndarray)</code> <p>Features, array of shape (n_nodes, n_features).</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>IsoGraphKernel</code> <p>The fitted estimator.</p> Source code in <code>ikpykit/graph/_isographkernel.py</code> <pre><code>def fit(\n    self,\n    features: Union[sp.csr_matrix, np.ndarray],\n):\n    \"\"\"Fit the model on data X.\n\n    Parameters\n    ----------\n    features : sparse.csr_matrix, np.ndarray\n        Features, array of shape (n_nodes, n_features).\n\n    Returns\n    -------\n    self : IsoGraphKernel\n        The fitted estimator.\n    \"\"\"\n    features = check_array(features)\n    self.iso_kernel_ = IsoKernel(\n        self.method, self.n_estimators, self.max_samples, self.random_state\n    )\n    self.iso_kernel_ = self.iso_kernel_.fit(features)\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"api/graph/IsoGraphKernel.html#ikpykit.graph.IsoGraphKernel.similarity","title":"similarity","text":"<pre><code>similarity(X, dense_output=True)\n</code></pre> <p>Compute the isolation kernel similarity matrix of X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[csr_matrix, ndarray]</code> <p>The input instances or pre-computed embeddings.</p> required <code>dense_output</code> <code>bool</code> <p>Whether to return dense matrix of output.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>similarity</code> <code>array-like of shape (n_instances, n_instances)</code> <p>The similarity matrix organized as an n_instances * n_instances matrix.</p> Source code in <code>ikpykit/graph/_isographkernel.py</code> <pre><code>def similarity(\n    self, X: Union[sp.csr_matrix, np.ndarray], dense_output: bool = True\n) -&gt; Union[sp.csr_matrix, np.ndarray]:\n    \"\"\"Compute the isolation kernel similarity matrix of X.\n\n    Parameters\n    ----------\n    X: array-like of shape (n_instances, n_features)\n        The input instances or pre-computed embeddings.\n    dense_output: bool, default=True\n        Whether to return dense matrix of output.\n\n    Returns\n    -------\n    similarity : array-like of shape (n_instances, n_instances)\n        The similarity matrix organized as an n_instances * n_instances matrix.\n    \"\"\"\n    check_is_fitted(self)\n    X = check_array(X)\n\n    return safe_sparse_dot(X, X.T, dense_output=dense_output) / self.n_estimators\n</code></pre>"},{"location":"api/graph/IsoGraphKernel.html#ikpykit.graph.IsoGraphKernel.transform","title":"transform","text":"<pre><code>transform(adjacency, features, h, dense_output=False)\n</code></pre> <p>Compute the isolation kernel feature of a graph.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency</code> <code>Union[csr_matrix, ndarray]</code> <p>Adjacency matrix of the graph.</p> required <code>features</code> <code>(csr_matrix, ndarray)</code> <p>Features, array of shape (n_nodes, n_features).</p> required <code>h</code> <code>int</code> <p>The number of iterations for Weisfeiler\u2013Lehman embedding.</p> required <code>dense_output</code> <code>bool</code> <p>Whether to return a dense array.</p> <code>False</code> <p>Returns:</p> Type Description <code>The finite binary features based on the kernel feature map.</code> <code>The features are organized as an n_instances by h+1*psi*t matrix.</code> Source code in <code>ikpykit/graph/_isographkernel.py</code> <pre><code>def transform(\n    self,\n    adjacency: Union[sp.csr_matrix, np.ndarray],\n    features: Union[sp.csr_matrix, np.ndarray],\n    h: int,\n    dense_output: bool = False,\n) -&gt; Union[sp.csr_matrix, np.ndarray]:\n    \"\"\"Compute the isolation kernel feature of a graph.\n\n    Parameters\n    ----------\n    adjacency : Union[sp.csr_matrix, np.ndarray]\n        Adjacency matrix of the graph.\n    features : sparse.csr_matrix, np.ndarray\n        Features, array of shape (n_nodes, n_features).\n    h : int\n        The number of iterations for Weisfeiler\u2013Lehman embedding.\n    dense_output : bool, default=False\n        Whether to return a dense array.\n\n    Returns\n    -------\n    The finite binary features based on the kernel feature map.\n    The features are organized as an n_instances by h+1*psi*t matrix.\n    \"\"\"\n    check_is_fitted(self)\n    features = check_array(features)\n    adjacency = check_format(adjacency)\n    X_trans = self.iso_kernel_.transform(features)\n    embedding = self._wlembedding(adjacency, X_trans, h)\n\n    if dense_output:\n        if sp.issparse(embedding) and hasattr(embedding, \"toarray\"):\n            return embedding.toarray()\n        else:\n            warn(\"The IsoKernel transform output is already dense.\")\n    return embedding\n</code></pre>"},{"location":"api/graph/IsoGraphKernel.html#ikpykit.graph.IsoGraphKernel.fit_transform","title":"fit_transform","text":"<pre><code>fit_transform(adjacency, features, h, dense_output=False)\n</code></pre> <p>Fit the model on data X and transform X.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency</code> <code>Union[csr_matrix, ndarray]</code> <p>Adjacency matrix of the graph.</p> required <code>features</code> <code>(csr_matrix, ndarray)</code> <p>Features, array of shape (n_nodes, n_features).</p> required <code>h</code> <code>int</code> <p>The number of iterations for Weisfeiler\u2013Lehman embedding.</p> required <code>dense_output</code> <code>bool</code> <p>Whether to return a dense array.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>embedding</code> <code>Union[csr_matrix, ndarray]</code> <p>Transformed array.</p> Source code in <code>ikpykit/graph/_isographkernel.py</code> <pre><code>def fit_transform(\n    self,\n    adjacency: Union[np.ndarray, sp.csr_matrix],\n    features: Union[sp.csr_matrix, np.ndarray],\n    h: int,\n    dense_output: bool = False,\n) -&gt; Union[sp.csr_matrix, np.ndarray]:\n    \"\"\"Fit the model on data X and transform X.\n\n    Parameters\n    ----------\n    adjacency : Union[sp.csr_matrix, np.ndarray]\n        Adjacency matrix of the graph.\n    features : sparse.csr_matrix, np.ndarray\n        Features, array of shape (n_nodes, n_features).\n    h : int\n        The number of iterations for Weisfeiler\u2013Lehman embedding.\n    dense_output : bool, default=False\n        Whether to return a dense array.\n\n    Returns\n    -------\n    embedding : Union[sp.csr_matrix, np.ndarray]\n        Transformed array.\n    \"\"\"\n    self.fit(features)\n    return self.transform(adjacency, features, h, dense_output)\n</code></pre>"},{"location":"api/graph/ikgod.html","title":"IKGOD","text":""},{"location":"api/graph/ikgod.html#ikpykit.graph.IKGOD","title":"ikpykit.graph.IKGOD","text":"<pre><code>IKGOD(\n    n_estimators=200,\n    max_samples=\"auto\",\n    contamination=\"auto\",\n    method=\"inne\",\n    random_state=None,\n    h=3,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code></p> <p>Isolation-based Graph Anomaly Detection using kernel embeddings.</p> <p>This algorithm detects anomalies in graphs by using isolation kernels on subgraph features. It combines graph structure and node features to identify outliers.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <code>int</code> <p>Number of isolation estimators in the ensemble.</p> <code>200</code> <code>max_samples</code> <code>(int, float or auto)</code> <p>Number of samples to draw for training each base estimator: - If int, draw <code>max_samples</code> samples - If float, draw <code>max_samples * X.shape[0]</code> samples - If \"auto\", use <code>min(16, n_samples)</code></p> <code>\"auto\"</code> <code>contamination</code> <code>float or auto</code> <p>Expected proportion of outliers in the data: - If \"auto\", threshold is set at -0.5 as in the original paper - If float, must be in range (0, 0.5]</p> <code>\"auto\"</code> <code>method</code> <code>(inne, anne, auto)</code> <p>Isolation method to use. The original algorithm uses \"inne\".</p> <code>\"inne\"</code> <code>random_state</code> <code>(int, RandomState or None)</code> <p>Controls randomness for reproducibility.</p> <code>None</code> <code>h</code> <code>int</code> <p>Maximum hop distance for subgraph extraction.</p> <code>3</code> <p>Attributes:</p> Name Type Description <code>max_samples_</code> <code>int</code> <p>Actual number of samples used</p> <code>embedding_</code> <code>array of shape (n_samples, n_features)</code> <p>Learned subgraph embeddings</p> <code>offset_</code> <code>float</code> <p>Threshold for determining outliers</p> <code>is_fitted_</code> <code>bool</code> <p>Whether the model has been fitted</p> References <p>.. [1] Zhong Zhuang, Kai Ming Ting, Guansong Pang, Shuaibin Song (2023).    Subgraph Centralization: A Necessary Step for Graph Anomaly Detection.    Proceedings of The SIAM Conference on Data Mining.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.graph import IKGOD\n&gt;&gt;&gt; import scipy.sparse as sp\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Create adjacency matrix and features\n&gt;&gt;&gt; adj = sp.csr_matrix([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n&gt;&gt;&gt; features = np.array([[0.1, 0.2], [0.3, 0.4], [5.0, 6.0]])\n&gt;&gt;&gt; # Fit model\n&gt;&gt;&gt; model = IKGOD(n_estimators=100, h=2).fit(adj, features)\n&gt;&gt;&gt; # Predict outliers\n&gt;&gt;&gt; lables = model.predict(features)\n</code></pre> Source code in <code>ikpykit/graph/_ikgod.py</code> <pre><code>def __init__(\n    self,\n    n_estimators=200,\n    max_samples=\"auto\",\n    contamination=\"auto\",\n    method=\"inne\",\n    random_state=None,\n    h=3,\n):\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.random_state = random_state\n    self.contamination = contamination\n    self.method = method\n    self.h = h\n</code></pre>"},{"location":"api/graph/ikgod.html#ikpykit.graph.IKGOD.fit","title":"fit","text":"<pre><code>fit(adjacency, features, y=None)\n</code></pre> <p>Fit the IKGOD model.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency</code> <code>array-like or sparse matrix of shape (n_samples, n_samples)</code> <p>Adjacency matrix of the graph</p> required <code>features</code> <code>array-like of shape (n_samples, n_features)</code> <p>Node features</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>ikpykit/graph/_ikgod.py</code> <pre><code>def fit(self, adjacency, features, y=None):\n    \"\"\"Fit the IKGOD model.\n\n    Parameters\n    ----------\n    adjacency : array-like or sparse matrix of shape (n_samples, n_samples)\n        Adjacency matrix of the graph\n\n    features : array-like of shape (n_samples, n_features)\n        Node features\n\n    y : Ignored\n        Not used, present for API consistency.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n    # Check and format inputs\n    adjacency = check_format(adjacency)\n    features = check_array(features, accept_sparse=False)\n\n    n_samples = features.shape[0]\n\n    # Determine max_samples\n    if isinstance(self.max_samples, str):\n        if self.max_samples == \"auto\":\n            max_samples = min(16, n_samples)\n        else:\n            raise ValueError(\n                f\"max_samples '{self.max_samples}' is not supported. \"\n                f'Valid choices are: \"auto\", int or float'\n            )\n    elif isinstance(self.max_samples, numbers.Integral):\n        if self.max_samples &gt; n_samples:\n            warn(\n                f\"max_samples ({self.max_samples}) is greater than the \"\n                f\"total number of samples ({n_samples}). max_samples \"\n                f\"will be set to n_samples for estimation.\"\n            )\n            max_samples = n_samples\n        else:\n            max_samples = self.max_samples\n    else:  # float\n        if not 0.0 &lt; self.max_samples &lt;= 1.0:\n            raise ValueError(\n                f\"max_samples must be in (0, 1], got {self.max_samples}\"\n            )\n        max_samples = int(self.max_samples * n_samples)\n\n    self.max_samples_ = max_samples\n\n    # Fit the model\n    self._fit(adjacency, features)\n    self.is_fitted_ = True\n\n    # Set contamination threshold\n    if self.contamination != \"auto\":\n        if not (0.0 &lt; self.contamination &lt;= 0.5):\n            raise ValueError(\n                f\"contamination must be in (0, 0.5], got: {self.contamination}\"\n            )\n\n    if self.contamination == \"auto\":\n        # 0.5 plays a special role as described in the original paper\n        self.offset_ = -0.5\n    else:\n        # Set threshold based on contamination parameter\n        self.offset_ = np.percentile(\n            self.score_samples(features), 100.0 * self.contamination\n        )\n\n    return self\n</code></pre>"},{"location":"api/graph/ikgod.html#ikpykit.graph.IKGOD.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict outliers in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples</p> required <p>Returns:</p> Name Type Description <code>is_inlier</code> <code>ndarray of shape (n_samples,)</code> <p>+1 for inliers, -1 for outliers</p> Source code in <code>ikpykit/graph/_ikgod.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict outliers in X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples\n\n    Returns\n    -------\n    is_inlier : ndarray of shape (n_samples,)\n        +1 for inliers, -1 for outliers\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    decision_func = self.decision_function(X)\n    is_inlier = np.ones_like(decision_func, dtype=int)\n    is_inlier[decision_func &lt; 0] = -1\n    return is_inlier\n</code></pre>"},{"location":"api/graph/ikgod.html#ikpykit.graph.IKGOD.decision_function","title":"decision_function","text":"<pre><code>decision_function(X)\n</code></pre> <p>Compute decision function.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>Decision scores. Negative scores represent outliers.</p> Source code in <code>ikpykit/graph/_ikgod.py</code> <pre><code>def decision_function(self, X):\n    \"\"\"Compute decision function.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        Decision scores. Negative scores represent outliers.\n    \"\"\"\n    return self.score_samples(X) - self.offset_\n</code></pre>"},{"location":"api/graph/ikgod.html#ikpykit.graph.IKGOD.score_samples","title":"score_samples","text":"<pre><code>score_samples(X)\n</code></pre> <p>Compute anomaly scores for samples.</p> <p>Lower scores indicate more anomalous points.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input samples</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_samples,)</code> <p>Anomaly scores. Lower values indicate more anomalous points.</p> Source code in <code>ikpykit/graph/_ikgod.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"Compute anomaly scores for samples.\n\n    Lower scores indicate more anomalous points.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples\n\n    Returns\n    -------\n    scores : ndarray of shape (n_samples,)\n        Anomaly scores. Lower values indicate more anomalous points.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    X = check_array(X, accept_sparse=False)\n    kme = self._kernel_mean_embedding(self.embedding_)\n    scores = safe_sparse_dot(self.embedding_, kme.T).A1\n    return -scores\n</code></pre>"},{"location":"api/group/ikgad.html","title":"IKGAD","text":""},{"location":"api/group/ikgad.html#ikpykit.group.IKGAD","title":"ikpykit.group.IKGAD","text":"<pre><code>IKGAD(\n    n_estimators_1=200,\n    max_samples_1=\"auto\",\n    n_estimators_2=200,\n    max_samples_2=\"auto\",\n    method=\"inne\",\n    contamination=\"auto\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>Isolation Kernel-based Group Anomaly Detection.</p> <p>IKGAD applies isolation kernel techniques to detect anomalies in groups of data points. It leverages a two-step approach: first transforming the data using an isolation kernel, then calculating kernel mean embeddings for each group to detect anomalous groups. The algorithm is effective for detecting both global and local group anomalies.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators_1</code> <code>int</code> <p>The number of base estimators in the first-level ensemble.</p> <code>200</code> <code>max_samples_1</code> <code>int, float, or \"auto\"</code> <p>The number of samples to draw for training each first-level base estimator:</p> <ul> <li>If int, draws exactly <code>max_samples_1</code> samples</li> <li>If float, draws <code>max_samples_1 * X.shape[0]</code> samples</li> <li>If \"auto\", uses <code>min(8, n_samples)</code></li> </ul> <code>\"auto\"</code> <code>n_estimators_2</code> <code>int</code> <p>The number of base estimators in the second-level ensemble.</p> <code>200</code> <code>max_samples_2</code> <code>int, float, or \"auto\"</code> <p>The number of samples to draw for training each second-level base estimator:</p> <ul> <li>If int, draws exactly <code>max_samples_2</code> samples</li> <li>If float, draws <code>max_samples_2 * X.shape[0]</code> samples</li> <li>If \"auto\", uses <code>min(8, n_samples)</code></li> </ul> <code>\"auto\"</code> <code>method</code> <code>(inne, anne, auto)</code> <p>Isolation method to use. The \"inne\" option corresponds to the approach described in the original paper.</p> <code>\"inne\"</code> <code>contamination</code> <code>auto or float</code> <p>Proportion of outliers in the data set:</p> <ul> <li>If \"auto\", the threshold is determined as in the original paper</li> <li>If float, should be in range (0, 0.5]</li> </ul> <code>\"auto\"</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the random seed for reproducibility.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>iso_kernel_1_</code> <code>IsoKernel</code> <p>First-level trained isolation kernel.</p> <code>offset_</code> <code>float</code> <p>Decision threshold for outlier detection.</p> References <p>.. [1] Kai Ming Ting, Bi-Cun Xu, Washio Takashi, Zhi-Hua Zhou (2022).    Isolation Distributional Kernel: A new tool for kernel based point and group anomaly detections.    IEEE Transactions on Knowledge and Data Engineering.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.group import IKGAD\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X =[[[1.0, 1.1], [1.2, 1.3]], [[1.3, 1.2], [1.1, 1.0]], [[1.0, 1.2], [1.4, 1.3]], [[5.0, 5.1], [5.2, 5.3]]]\n&gt;&gt;&gt; clf = IKGAD(max_samples_1=2, max_samples_2=2, contamination=0.25, random_state=42)\n&gt;&gt;&gt; clf = clf.fit(X)\n&gt;&gt;&gt; clf.predict(X)\narray([ 1,  1,  1, -1])\n</code></pre> Source code in <code>ikpykit/group/anomaly/_ikgad.py</code> <pre><code>def __init__(\n    self,\n    n_estimators_1=200,\n    max_samples_1=\"auto\",\n    n_estimators_2=200,\n    max_samples_2=\"auto\",\n    method=\"inne\",\n    contamination=\"auto\",\n    random_state=None,\n):\n    self.n_estimators_1 = n_estimators_1\n    self.max_samples_1 = max_samples_1\n    self.n_estimators_2 = n_estimators_2\n    self.max_samples_2 = max_samples_2\n    self.random_state = random_state\n    self.contamination = contamination\n    self.method = method\n</code></pre>"},{"location":"api/group/ikgad.html#ikpykit.group.IKGAD.fit","title":"fit","text":"<pre><code>fit(X)\n</code></pre> <p>Fit the IKGAD model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_groups, n_samples, n_features)</code> <p>The input data, where n_groups is the number of groups, n_samples is the number of instances per group, and n_features is the number of features.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Notes <p>Sets the <code>is_fitted_</code> attribute to <code>True</code>.</p> Source code in <code>ikpykit/group/anomaly/_ikgad.py</code> <pre><code>def fit(self, X):\n    \"\"\"Fit the IKGAD model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_groups, n_samples, n_features)\n        The input data, where n_groups is the number of groups,\n        n_samples is the number of instances per group, and\n        n_features is the number of features.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n\n    Notes\n    -----\n    Sets the `is_fitted_` attribute to `True`.\n    \"\"\"\n    # Validate input data\n    X = check_format(X)\n    # Fit the model\n    self._fit(X)\n    self.is_fitted_ = True\n\n    # Set threshold\n    if self.contamination != \"auto\":\n        if not (0.0 &lt; self.contamination &lt;= 0.5):\n            raise ValueError(\n                f\"contamination must be in (0, 0.5], got: {self.contamination}\"\n            )\n        # Define threshold based on contamination parameter\n        self.offset_ = np.percentile(\n            self.score_samples(X), 100.0 * self.contamination\n        )\n    else:\n        # Use default threshold as described in the original paper\n        self.offset_ = -0.5\n\n    return self\n</code></pre>"},{"location":"api/group/ikgad.html#ikpykit.group.IKGAD.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict if groups are outliers or inliers.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_groups, n_samples, n_features)</code> <p>The input groups to evaluate</p> required <p>Returns:</p> Name Type Description <code>is_inlier</code> <code>ndarray of shape (n_groups,)</code> <p>For each group, returns whether it is an inlier (+1) or outlier (-1) according to the fitted model.</p> Source code in <code>ikpykit/group/anomaly/_ikgad.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict if groups are outliers or inliers.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_groups, n_samples, n_features)\n        The input groups to evaluate\n\n    Returns\n    -------\n    is_inlier : ndarray of shape (n_groups,)\n        For each group, returns whether it is an inlier (+1) or\n        outlier (-1) according to the fitted model.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    decision_func = self.decision_function(X)\n    is_inlier = np.ones_like(decision_func, dtype=int)\n    is_inlier[decision_func &lt; 0] = -1\n    return is_inlier\n</code></pre>"},{"location":"api/group/ikgad.html#ikpykit.group.IKGAD.decision_function","title":"decision_function","text":"<pre><code>decision_function(X)\n</code></pre> <p>Compute decision scores for groups.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_groups, n_samples, n_features)</code> <p>The input groups to evaluate</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_groups,)</code> <p>Decision scores. Negative scores represent outliers, positive scores represent inliers.</p> Source code in <code>ikpykit/group/anomaly/_ikgad.py</code> <pre><code>def decision_function(self, X):\n    \"\"\"Compute decision scores for groups.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_groups, n_samples, n_features)\n        The input groups to evaluate\n\n    Returns\n    -------\n    scores : ndarray of shape (n_groups,)\n        Decision scores. Negative scores represent outliers,\n        positive scores represent inliers.\n    \"\"\"\n    return self.score_samples(X) - self.offset_\n</code></pre>"},{"location":"api/group/ikgad.html#ikpykit.group.IKGAD.score_samples","title":"score_samples","text":"<pre><code>score_samples(X)\n</code></pre> <p>Compute anomaly scores for groups.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_groups, n_samples, n_features)</code> <p>The input groups to evaluate</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_groups,)</code> <p>Anomaly scores where lower values indicate more anomalous groups.</p> Source code in <code>ikpykit/group/anomaly/_ikgad.py</code> <pre><code>def score_samples(self, X):\n    \"\"\"Compute anomaly scores for groups.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_groups, n_samples, n_features)\n        The input groups to evaluate\n\n    Returns\n    -------\n    scores : ndarray of shape (n_groups,)\n        Anomaly scores where lower values indicate more anomalous groups.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    X = check_format(X)\n\n    X_full = np.vstack(X)\n    # Create kernel mean embeddings for each group\n    split_idx = np.cumsum([len(x) for x in X])\n    X_trans = self.iso_kernel_1_.transform(X_full)\n    group_embeddings = np.split(X_trans.toarray(), split_idx[:-1], axis=0)\n    X_embeddings = np.asarray(\n        [\n            self._kernel_mean_embedding(x, self.n_estimators_1)\n            for x in group_embeddings\n        ]\n    )\n\n    # Second level isolation kernel on the embeddings\n    iso_kernel_2 = IsoKernel(\n        n_estimators=self.n_estimators_2,\n        max_samples=self.max_samples_2,\n        random_state=self.random_state,\n        method=self.method,\n    )\n\n    X_trans = iso_kernel_2.fit_transform(X_embeddings)\n    kme = self._kernel_mean_embedding(X_trans, self.n_estimators_2)\n\n    # For sparse matrices, .A1 converts to 1D array\n    if hasattr(X_trans, \"A1\"):\n        scores = safe_sparse_dot(X_trans, kme.T).A1\n    else:\n        scores = safe_sparse_dot(X_trans, kme.T)\n        if hasattr(scores, \"A1\"):\n            scores = scores.A1\n        elif scores.ndim &gt; 1:\n            scores = scores.ravel()\n\n    return -scores\n</code></pre>"},{"location":"api/kernel/isolation_dis_kernel.html","title":"Isolation Distribution Kernel","text":""},{"location":"api/kernel/isolation_dis_kernel.html#ikpykit.kernel.IsoDisKernel","title":"ikpykit.kernel.IsoDisKernel","text":"<pre><code>IsoDisKernel(\n    method=\"anne\",\n    n_estimators=200,\n    max_samples=\"auto\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Isolation Distributional Kernel is a new way to measure the similarity between two distributions.</p> <p>It addresses two key issues of kernel mean embedding, where the kernel employed has: (i) a feature map with intractable dimensionality which leads to high computational cost; and (ii) data independency which leads to poor detection accuracy in anomaly detection.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to compute the isolation kernel feature. The available methods are: <code>anne</code>, <code>inne</code>, and <code>iforest</code>.</p> <code>\"anne\"</code> <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the ensemble.</p> <code>200</code> <code>max_samples</code> <code>int</code> <p>The number of samples to draw from X to train each base estimator.</p> <pre><code>- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples` * X.shape[0]` samples.\n- If \"auto\", then `max_samples=min(8, n_samples)`.\n</code></pre> <code>\"auto\"</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.</p> <p>Pass an int for reproducible results across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> References <p>.. [1] Kai Ming Ting, Bi-Cun Xu, Takashi Washio, and Zhi-Hua Zhou. 2020. \"Isolation Distributional Kernel: A New Tool for Kernel based Anomaly Detection\". In Proceedings of the 26<sup>th</sup> ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD '20). Association for Computing Machinery, New York, NY, USA, 198-206.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.kernel import IsoDisKernel\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = [[0.4,0.3], [0.3,0.8], [0.5,0.4], [0.5,0.1]]\n&gt;&gt;&gt; idk = IsoDisKernel(max_samples=3,).fit(X)\n&gt;&gt;&gt; D_i = [[0.4,0.3], [0.3,0.8]]\n&gt;&gt;&gt; D_j = [[0.5, 0.4], [0.5, 0.1]]\n&gt;&gt;&gt; idk.similarity(D_j, D_j)\n1.0\n</code></pre> Source code in <code>ikpykit/kernel/_isodiskernel.py</code> <pre><code>def __init__(\n    self, method=\"anne\", n_estimators=200, max_samples=\"auto\", random_state=None\n) -&gt; None:\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.random_state = random_state\n    self.method = method\n</code></pre>"},{"location":"api/kernel/isolation_dis_kernel.html#ikpykit.kernel.IsoDisKernel.fit","title":"fit","text":"<pre><code>fit(X)\n</code></pre> <p>Fit the model on data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array of shape (n_samples, n_features)</code> <p>The input instances.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> Source code in <code>ikpykit/kernel/_isodiskernel.py</code> <pre><code>def fit(self, X):\n    \"\"\"Fit the model on data X.\n    Parameters\n    ----------\n    X : np.array of shape (n_samples, n_features)\n        The input instances.\n    Returns\n    -------\n    self : object\n    \"\"\"\n    X = check_array(X)\n    iso_kernel = IsoKernel(\n        self.method, self.n_estimators, self.max_samples, self.random_state\n    )\n    self.iso_kernel_ = iso_kernel.fit(X)\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"api/kernel/isolation_dis_kernel.html#ikpykit.kernel.IsoDisKernel.kernel_mean","title":"kernel_mean","text":"<pre><code>kernel_mean(X)\n</code></pre> <p>Compute the kernel mean embedding of X.</p> Source code in <code>ikpykit/kernel/_isodiskernel.py</code> <pre><code>def kernel_mean(self, X):\n    \"\"\"Compute the kernel mean embedding of X.\"\"\"\n    if sp.issparse(X):\n        return np.asarray(X.mean(axis=0)).ravel()\n    return np.mean(X, axis=0)\n</code></pre>"},{"location":"api/kernel/isolation_dis_kernel.html#ikpykit.kernel.IsoDisKernel.similarity","title":"similarity","text":"<pre><code>similarity(D_i, D_j, is_normalize=True)\n</code></pre> <p>Compute the isolation distribution kernel of D_i and D_j.</p> <p>Parameters:</p> Name Type Description Default <code>D_i</code> <p>The input instances.</p> required <code>D_j</code> <p>The input instances.</p> required <code>is_normalize</code> <code>True</code> <p>Returns:</p> Type Description <code>The Isolation distribution similarity of given two dataset.</code> Source code in <code>ikpykit/kernel/_isodiskernel.py</code> <pre><code>def similarity(self, D_i, D_j, is_normalize=True):\n    \"\"\"Compute the isolation distribution kernel of D_i and D_j.\n    Parameters\n    ----------\n    D_i: array-like of shape (n_instances, n_features)\n        The input instances.\n    D_j: array-like of shape (n_instances, n_features)\n        The input instances.\n    is_normalize: whether return the normalized similarity matrix ranged of [0,1]. Default: False\n    Returns\n    -------\n    The Isolation distribution similarity of given two dataset.\n    \"\"\"\n    emb_D_i, emb_D_j = self.transform(D_i, D_j)\n    kme_D_i, kme_D_j = self.kernel_mean(emb_D_i), self.kernel_mean(emb_D_j)\n    return self.kme_similarity(kme_D_i, kme_D_j, is_normalize=is_normalize)\n</code></pre>"},{"location":"api/kernel/isolation_dis_kernel.html#ikpykit.kernel.IsoDisKernel.transform","title":"transform","text":"<pre><code>transform(D_i, D_j)\n</code></pre> <p>Compute the isolation kernel feature of D_i and D_j.</p> <p>Parameters:</p> Name Type Description Default <code>D_i</code> <p>The input instances.</p> required <code>D_j</code> <p>The input instances.</p> required <p>Returns:</p> Type Description <code>The finite binary features based on the kernel feature map.</code> <code>The features are organised as a n_instances by psi*t matrix.</code> Source code in <code>ikpykit/kernel/_isodiskernel.py</code> <pre><code>def transform(self, D_i, D_j):\n    \"\"\"Compute the isolation kernel feature of D_i and D_j.\n    Parameters\n    ----------\n    D_i: array-like of shape (n_instances, n_features)\n        The input instances.\n    D_j: array-like of shape (n_instances, n_features)\n        The input instances.\n    Returns\n    -------\n    The finite binary features based on the kernel feature map.\n    The features are organised as a n_instances by psi*t matrix.\n    \"\"\"\n    check_is_fitted(self)\n    D_i = check_array(D_i)\n    D_j = check_array(D_j)\n    return self.iso_kernel_.transform(D_i), self.iso_kernel_.transform(D_j)\n</code></pre>"},{"location":"api/kernel/isolation_kernel.html","title":"Isolation Kernel","text":""},{"location":"api/kernel/isolation_kernel.html#ikpykit.kernel.IsoKernel","title":"ikpykit.kernel.IsoKernel","text":"<pre><code>IsoKernel(\n    method=\"anne\",\n    n_estimators=200,\n    max_samples=\"auto\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Isolation Kernel.</p> <p>Build Isolation Kernel feature vector representations via the feature map for a given dataset.</p> <p>Isolation kernel is a data dependent kernel measure that is adaptive to local data distribution and has more flexibility in capturing the characteristics of the local data distribution. It has been shown promising performance on density and distance-based classification and clustering problems.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to compute the isolation kernel feature. The available methods are: <code>anne</code>, <code>inne</code>, and <code>iforest</code>.</p> <code>\"anne\"</code> <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the ensemble.</p> <code>200</code> <code>max_samples</code> <code>int</code> <p>The number of samples to draw from X to train each base estimator.</p> <pre><code>- If int, then draw `max_samples` samples.\n- If float, then draw `max_samples` * X.shape[0]` samples.\n- If \"auto\", then `max_samples=min(8, n_samples)`.\n</code></pre> <code>\"auto\"</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.</p> <p>Pass an int for reproducible results across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <code>None</code> References <p>.. [1] Qin, X., Ting, K.M., Zhu, Y. and Lee, V.C. \"Nearest-neighbour-induced isolation similarity and its impact on density-based clustering\". In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, 2019, July, pp. 4755-4762</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.kernel import IsoKernel\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = [[0.4,0.3], [0.3,0.8], [0.5, 0.4], [0.5, 0.1]]\n&gt;&gt;&gt; ik = IsoKernel().fit(X)\n&gt;&gt;&gt; X_trans = ik.transform(X)\n&gt;&gt;&gt; X_sim = ik.similarity(X)\n</code></pre> Source code in <code>ikpykit/kernel/_isokernel.py</code> <pre><code>def __init__(\n    self, method=\"anne\", n_estimators=200, max_samples=\"auto\", random_state=None\n) -&gt; None:\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.random_state = random_state\n    self.method = method\n</code></pre>"},{"location":"api/kernel/isolation_kernel.html#ikpykit.kernel.IsoKernel.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the model on data X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array of shape (n_samples, n_features)</code> <p>The input instances.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> Source code in <code>ikpykit/kernel/_isokernel.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the model on data X.\n    Parameters\n    ----------\n    X : np.array of shape (n_samples, n_features)\n        The input instances.\n    Returns\n    -------\n    self : object\n    \"\"\"\n\n    X = check_array(X)\n    n_samples = X.shape[0]\n    if isinstance(self.max_samples, str):\n        if self.max_samples == \"auto\":\n            max_samples = min(16, n_samples)\n        else:\n            raise ValueError(\n                \"max_samples (%s) is not supported.\"\n                'Valid choices are: \"auto\", int or'\n                \"float\" % self.max_samples\n            )\n    elif isinstance(self.max_samples, numbers.Integral):\n        if self.max_samples &gt; n_samples:\n            warn(\n                \"max_samples (%s) is greater than the \"\n                \"total number of samples (%s). max_samples \"\n                \"will be set to n_samples for estimation.\"\n                % (self.max_samples, n_samples)\n            )\n            max_samples = n_samples\n        else:\n            max_samples = self.max_samples\n    else:  # float\n        if not 0.0 &lt; self.max_samples &lt;= 1.0:\n            raise ValueError(\n                \"max_samples must be in (0, 1], got %r\" % self.max_samples\n            )\n        max_samples = int(self.max_samples * X.shape[0])\n    self.max_samples_ = max_samples\n\n    if self.method == \"anne\":\n        self.iso_kernel_ = IK_ANNE(\n            self.n_estimators, self.max_samples_, self.random_state\n        )\n    elif self.method == \"inne\":\n        self.iso_kernel_ = IK_INNE(\n            self.n_estimators, self.max_samples_, self.random_state\n        )\n    elif self.method == \"iforest\":\n        self.iso_kernel_ = IK_IForest(\n            self.n_estimators, self.max_samples_, self.random_state\n        )\n    else:\n        raise ValueError(\n            \"method (%s) is not supported.\"\n            'Valid choices are: \"anne\", \"inne\" or \"iforest\"' % self.method\n        )\n\n    self.iso_kernel_.fit(X)\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"api/kernel/isolation_kernel.html#ikpykit.kernel.IsoKernel.similarity","title":"similarity","text":"<pre><code>similarity(X, dense_output=True)\n</code></pre> <p>Compute the isolation kernel similarity matrix of X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input instances.</p> required <code>dense_output</code> <p>Whether to return dense matrix of output.</p> <code>True</code> <p>Returns:</p> Type Description <code>The simalarity matrix are organised as a n_instances * n_instances matrix.</code> Source code in <code>ikpykit/kernel/_isokernel.py</code> <pre><code>def similarity(self, X, dense_output=True):\n    \"\"\"Compute the isolation kernel similarity matrix of X.\n    Parameters\n    ----------\n    X: array-like of shape (n_instances, n_features)\n        The input instances.\n    dense_output: bool, default=True\n        Whether to return dense matrix of output.\n    Returns\n    -------\n    The simalarity matrix are organised as a n_instances * n_instances matrix.\n    \"\"\"\n    check_is_fitted(self)\n    X = check_array(X)\n    embed_X = self.transform(X)\n    return (\n        safe_sparse_dot(embed_X, embed_X.T, dense_output=dense_output)\n        / self.n_estimators\n    )\n</code></pre>"},{"location":"api/kernel/isolation_kernel.html#ikpykit.kernel.IsoKernel.transform","title":"transform","text":"<pre><code>transform(X, dense_output=False)\n</code></pre> <p>Compute the isolation kernel feature of X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>The input instances.</p> required <code>dense_output</code> <p>Whether to return dense matrix of output.</p> <code>False</code> <p>Returns:</p> Type Description <code>The finite binary features based on the kernel feature map.</code> <code>The features are organised as a n_instances by psi*t matrix.</code> Source code in <code>ikpykit/kernel/_isokernel.py</code> <pre><code>def transform(self, X, dense_output=False):\n    \"\"\"Compute the isolation kernel feature of X.\n    Parameters\n    ----------\n    X: array-like of shape (n_instances, n_features)\n        The input instances.\n    dense_output: bool, default=False\n        Whether to return dense matrix of output.\n    Returns\n    -------\n    The finite binary features based on the kernel feature map.\n    The features are organised as a n_instances by psi*t matrix.\n    \"\"\"\n\n    check_is_fitted(self)\n    X = check_array(X)\n    X_trans = self.iso_kernel_.transform(X)\n    if dense_output:\n        if sp.issparse(X_trans) and hasattr(X_trans, \"toarray\"):\n            return X_trans.toarray()\n        else:\n            warn(\"The IsoKernel transform output is already dense.\")\n    return X_trans\n</code></pre>"},{"location":"api/stream/icid.html","title":"ICID","text":""},{"location":"api/stream/icid.html#ikpykit.stream.ICID","title":"ikpykit.stream.ICID","text":"<pre><code>ICID(\n    n_estimators=200,\n    max_samples_list=[2, 4, 8, 16, 32, 64],\n    method=\"inne\",\n    stability_method=\"entropy\",\n    adjust_rate=0.1,\n    contamination=\"auto\",\n    window_size=10,\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code></p> <p>Isolate Change Interval Detection for monitoring data stream distribution changes.</p> <p>ICID (Isolate Change Interval Detection) is designed to detect intervals in a data stream where significant distribution changes occur. It leverages isolation-based methods to measure similarity between consecutive data windows, identifying points where the underlying distribution shifts. The algorithm adaptively selects the best sampling parameters for isolation kernels based on stability metrics.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the isolation distribution kernel.</p> <code>200</code> <code>max_samples_list</code> <code>list of int</code> <p>List of candidate values for max_samples parameter. The algorithm will select the value that yields the most stable isolation kernel.</p> <code>[2, 4, 8, 16, 32, 64]</code> <code>method</code> <code>(inne, anne)</code> <p>The isolation method to use for the kernel.</p> <pre><code>- 'inne': Isolation-based Nearest Neighbor Ensemble\n- 'anne': Approximate Nearest Neighbor Ensemble\n</code></pre> <code>'inne'</code> <code>stability_method</code> <code>(entropy, variance, mean)</code> <p>Method used to evaluate the stability of interval scores.</p> <pre><code>- 'entropy': Use information entropy as stability measure\n- 'variance': Use variance as stability measure\n- 'mean': Use mean value as stability measure\n</code></pre> <code>'entropy'</code> <code>window_size</code> <code>int</code> <p>The size of the sliding window for batch detection.</p> <code>10</code> <code>adjust_rate</code> <code>float</code> <p>Rate to adjust the threshold for anomaly detection based on standard deviation of interval scores.</p> <code>0.1</code> <code>contamination</code> <code>auto or float</code> <p>The proportion of outliers in the data set. Used when fitting to define the threshold on interval scores.</p> <code>'auto'</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the randomness of the estimator.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>best_iso_kernel_</code> <code>IsoDisKernel</code> <p>The fitted isolation kernel with the best stability score.</p> <code>best_stability_score_</code> <code>float</code> <p>The stability score of the best isolation kernel.</p> <code>interval_score_</code> <code>array-like of shape (n_intervals,)</code> <p>The dissimilarity scores between consecutive intervals.</p> <code>best_max_samples_</code> <code>int</code> <p>The max_samples parameter of the best isolation kernel.</p> <code>pre_interval_</code> <code>array - like</code> <p>The last interval from the training data, used for online prediction.</p> References <p>.. [1] Y. Cao, Y. Zhu, K. M. Ting, F. D. Salim, H. X. Li, L. Yang, G. Li (2024).        Detecting change intervals with isolation distributional kernel.        Journal of Artificial Intelligence Research, 79:273\u2013306.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.stream import ICID\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; X_normal1 = np.random.randn(50, 2)\n&gt;&gt;&gt; X_anomaly = np.random.randn(10, 2) * 5 + 10  # Different distribution\n&gt;&gt;&gt; X_normal2 = np.random.randn(20, 2)\n&gt;&gt;&gt; X = np.vstack([X_normal1, X_anomaly, X_normal2])\n&gt;&gt;&gt; icid = ICID(n_estimators=50, max_samples_list=[4, 8], window_size=10, random_state=42)\n&gt;&gt;&gt; # Batch predictions\n&gt;&gt;&gt; icid.fit_predict_batch(X)\narray([ 1,  1,  1,  1,  -1,  -1,  1])\n&gt;&gt;&gt; X_anomaly = np.random.randn(10, 2) * 5 + 10\n&gt;&gt;&gt; X_normal = np.random.randn(10, 2)\n&gt;&gt;&gt; # Predict on new data online\n&gt;&gt;&gt; icid.predict_online(X_normal)\n1\n&gt;&gt;&gt; icid.predict_online(X_anomaly)\n-1\n</code></pre> Source code in <code>ikpykit/stream/changedetect/_icid.py</code> <pre><code>def __init__(\n    self,\n    n_estimators=200,\n    max_samples_list=[2, 4, 8, 16, 32, 64],\n    method=\"inne\",\n    stability_method=\"entropy\",\n    adjust_rate=0.1,\n    contamination=\"auto\",\n    window_size=10,\n    random_state=None,\n):\n    self.n_estimators = n_estimators\n    self.max_samples_list = max_samples_list\n    self.method = method\n    self.stability_method = stability_method\n    self.contamination = contamination\n    self.window_size = window_size\n    self.random_state = random_state\n    self.adjust_rate = adjust_rate\n    self.best_iso_kernel_ = None\n    self.pre_interval_ = None\n    self.interval_score_ = None\n    self.best_stability_score_ = float(\"inf\")\n</code></pre>"},{"location":"api/stream/icid.html#ikpykit.stream.ICID.best_stability_score","title":"best_stability_score  <code>property</code>","text":"<pre><code>best_stability_score\n</code></pre> <p>Get the best stability score found during fitting.</p>"},{"location":"api/stream/icid.html#ikpykit.stream.ICID.best_iso_kernel","title":"best_iso_kernel  <code>property</code>","text":"<pre><code>best_iso_kernel\n</code></pre> <p>Get the isolation kernel with the best stability.</p>"},{"location":"api/stream/icid.html#ikpykit.stream.ICID.best_max_samples","title":"best_max_samples  <code>property</code>","text":"<pre><code>best_max_samples\n</code></pre> <p>Get the max_samples parameter of the best isolation kernel.</p>"},{"location":"api/stream/icid.html#ikpykit.stream.ICID.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the model on data X in batch mode.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array of shape (n_samples, n_features)</code> <p>The input instances.</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> Source code in <code>ikpykit/stream/changedetect/_icid.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"Fit the model on data X in batch mode.\n\n    Parameters\n    ----------\n    X : np.array of shape (n_samples, n_features)\n        The input instances.\n    Returns\n    -------\n    self : object\n    \"\"\"\n    X = check_array(X)\n    for max_samples in self.max_samples_list:\n        isodiskernel = IsoDisKernel(\n            n_estimators=self.n_estimators,\n            max_samples=max_samples,\n            random_state=self.random_state,\n            method=self.method,\n        )\n        isodiskernel.fit(X)\n        interval_scores = self._interval_score(X, isodiskernel, self.window_size)\n        stability_score = self._stability_score(interval_scores)\n        if stability_score &lt; self.best_stability_score_:\n            self.best_iso_kernel_ = isodiskernel\n            self.best_stability_score_ = stability_score\n            self.interval_score_ = interval_scores\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"api/stream/icid.html#ikpykit.stream.ICID.fit_predict_batch","title":"fit_predict_batch","text":"<pre><code>fit_predict_batch(X)\n</code></pre> <p>Fit the model on data X and predict anomalies in batch mode.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array of shape (n_samples, n_features)</code> <p>The input instances.</p> required <code>window_size</code> <code>int</code> <p>The size of the sliding window.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>is_inlier</code> <code>np.array of shape (n_intervals,)</code> <p>Returns 1 for inliers and -1 for outliers.</p> Source code in <code>ikpykit/stream/changedetect/_icid.py</code> <pre><code>def fit_predict_batch(self, X):\n    \"\"\"Fit the model on data X and predict anomalies in batch mode.\n\n    Parameters\n    ----------\n    X : np.array of shape (n_samples, n_features)\n        The input instances.\n    window_size : int, default=10\n        The size of the sliding window.\n\n    Returns\n    -------\n    is_inlier : np.array of shape (n_intervals,)\n        Returns 1 for inliers and -1 for outliers.\n    \"\"\"\n    self.fit(X)\n    is_inlier = np.ones(len(self.interval_score_), dtype=int)\n    threshold = self._determine_anomaly_bounds()\n    is_inlier[\n        self.interval_score_ &gt; threshold\n    ] = -1  # Higher scores indicate change\n    return is_inlier\n</code></pre>"},{"location":"api/stream/icid.html#ikpykit.stream.ICID.predict_online","title":"predict_online","text":"<pre><code>predict_online(X)\n</code></pre> <p>Predict if the new data represents a change from the previous interval.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array of shape (n_samples, n_features)</code> <p>The new data interval to evaluate.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>1 for normal (inlier), -1 for change detected (outlier)</code> Source code in <code>ikpykit/stream/changedetect/_icid.py</code> <pre><code>def predict_online(self, X):\n    \"\"\"Predict if the new data represents a change from the previous interval.\n\n    Parameters\n    ----------\n    X : np.array of shape (n_samples, n_features)\n        The new data interval to evaluate.\n\n    Returns\n    -------\n    int : 1 for normal (inlier), -1 for change detected (outlier)\n    \"\"\"\n    check_is_fitted(self, [\"best_iso_kernel_\", \"pre_interval_\", \"interval_score_\"])\n    X = check_array(X)\n    anomaly_score = 1.0 - self.best_iso_kernel_.similarity(self.pre_interval_, X)\n    self.interval_score_.append(anomaly_score)\n    self.pre_interval_ = X\n\n    threshold = self._determine_anomaly_bounds()\n    return 1 if anomaly_score &lt;= threshold else -1\n</code></pre>"},{"location":"api/stream/streakhc.html","title":"StreaKHC","text":""},{"location":"api/stream/streakhc.html#ikpykit.stream.STREAMKHC","title":"ikpykit.stream.STREAMKHC","text":"<pre><code>STREAMKHC(\n    method=\"anne\",\n    n_estimators=200,\n    max_samples=\"auto\",\n    max_leaf=5000,\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>Streaming Hierarchical Clustering Based on Point-Set Kernel.</p> <p>This algorithm performs hierarchical clustering on streaming data using isolation kernel techniques. It builds a tree structure that adapts as new data points arrive, allowing for efficient online clustering.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method used to calculate the Isolation Kernel. Possible values are 'inne' and 'anne'.</p> <code>\"anne\"</code> <code>n_estimators</code> <code>int</code> <p>The number of base estimators in the isolation kernel.</p> <code>200</code> <code>max_samples</code> <code>(str, int or float)</code> <p>The number of samples to draw from X to train each base estimator. - If int, then draw <code>max_samples</code> samples. - If float, then draw <code>max_samples * X.shape[0]</code> samples. - If \"auto\", then <code>max_samples=min(8, n_samples)</code>.</p> <code>\"auto\"</code> <code>max_leaf</code> <code>int</code> <p>Maximum number of data points to maintain in the clustering tree. When exceeded, the oldest points will be removed.</p> <code>5000</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the randomness of the estimator.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>tree_</code> <code>INODE</code> <p>The root node of the hierarchical clustering tree.</p> <code>iso_kernel_</code> <code>IsoKernel</code> <p>The isolation kernel used for data transformation.</p> <code>point_counter_</code> <code>int</code> <p>Counter tracking the total number of points processed.</p> <code>n_features_in_</code> <code>int</code> <p>Number of features seen during fit.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.stream import STREAMKHC\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Generate sample data\n&gt;&gt;&gt; X = np.random.rand(100, 10)  # 100 samples with 10 features\n&gt;&gt;&gt; y = np.random.randint(0, 3, size=100)  # Optional class labels\n&gt;&gt;&gt; # Initialize and fit the model with a batch\n&gt;&gt;&gt; clusterer = STREAMKHC(n_estimators=100, random_state=42)\n&gt;&gt;&gt; clusterer =  clusterer.fit(X, y)\n&gt;&gt;&gt; # Process new streaming data\n&gt;&gt;&gt; new_data = np.random.rand(1, 10)  # 10 new samples\n&gt;&gt;&gt; new_labels = np.random.randint(0, 3, size=1)  # Optional class labels\n&gt;&gt;&gt; clusterer = clusterer.fit_online(new_data, new_labels)\n&gt;&gt;&gt; # Calculate clustering purity (if labels were provided)\n&gt;&gt;&gt; purity = clusterer.get_purity()\n</code></pre> References <p>.. [1] Xin Han, Ye Zhu, Kai Ming Ting, De-Chuan Zhan, Gang Li (2022)        Streaming Hierarchical Clustering Based on Point-Set Kernel.        Proceedings of The ACM SIGKDD Conference on Knowledge Discovery and Data Mining.</p> Source code in <code>ikpykit/stream/cluster/_streakhc.py</code> <pre><code>def __init__(\n    self,\n    method: Literal[\"inne\", \"anne\"] = \"anne\",\n    n_estimators: int = 200,\n    max_samples: Union[Literal[\"auto\"], int, float] = \"auto\",\n    max_leaf: int = 5000,\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n):\n    self.method = method\n    self.n_estimators = n_estimators\n    self.max_samples = max_samples\n    self.max_leaf = max_leaf\n    self.random_state = random_state\n    self.tree_ = None\n    self.point_counter_ = 0\n    self.iso_kernel_ = None\n    self.n_features_in_ = None\n</code></pre>"},{"location":"api/stream/streakhc.html#ikpykit.stream.STREAMKHC.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the model with a batch of data points.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input data points.</p> required <code>y</code> <code>array-like of shape (n_samples,), optional (default=None)</code> <p>The labels of the data points. Not used in clustering processing, just for calculating purity. If not provided, the model will generate a tree with a single label.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>STREAMKHC</code> <p>Returns self.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid or data has incorrect shape.</p> Source code in <code>ikpykit/stream/cluster/_streakhc.py</code> <pre><code>def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; STREAMKHC:\n    \"\"\"Fit the model with a batch of data points.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input data points.\n    y : array-like of shape (n_samples,), optional (default=None)\n        The labels of the data points.\n        Not used in clustering processing, just for calculating purity.\n        If not provided, the model will generate a tree with a single label.\n\n    Returns\n    -------\n    self : STREAMKHC\n        Returns self.\n\n    Raises\n    ------\n    ValueError\n        If parameters are invalid or data has incorrect shape.\n    \"\"\"\n\n    if isinstance(self.max_leaf, int) and self.max_leaf &lt;= 0:\n        raise ValueError(f\"max_leaf must be positive, got {self.max_leaf}\")\n\n    # Process input data\n    X = check_array(X, accept_sparse=False)\n    if y is None:\n        y = np.ones(X.shape[0], dtype=np.int64)\n    else:\n        X, y = check_X_y(X, y, accept_sparse=False)\n\n    self.n_features_in_ = X.shape[1]\n    self._initialize_tree(X, y)\n    return self\n</code></pre>"},{"location":"api/stream/streakhc.html#ikpykit.stream.STREAMKHC.fit_online","title":"fit_online","text":"<pre><code>fit_online(X, y=None)\n</code></pre> <p>Fit the model with a stream of data points.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>The input data points.</p> required <code>y</code> <code>array-like of shape (n_samples,), optional (default=None)</code> <p>The labels of the data points. Not used in clustering processing, just for calculating purity. If not provided, the model will generate a tree with a single label.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>STREAMKHC</code> <p>Returns self.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the model has not been initialized with fit.</p> <code>ValueError</code> <p>If X has a different number of features than seen during fit.</p> Source code in <code>ikpykit/stream/cluster/_streakhc.py</code> <pre><code>def fit_online(self, X: np.ndarray, y: Optional[np.ndarray] = None) -&gt; STREAMKHC:\n    \"\"\"Fit the model with a stream of data points.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input data points.\n    y : array-like of shape (n_samples,), optional (default=None)\n        The labels of the data points.\n        Not used in clustering processing, just for calculating purity.\n        If not provided, the model will generate a tree with a single label.\n\n    Returns\n    -------\n    self : STREAMKHC\n        Returns self.\n\n    Raises\n    ------\n    NotFittedError\n        If the model has not been initialized with fit.\n    ValueError\n        If X has a different number of features than seen during fit.\n    \"\"\"\n    # Check if model is fitted\n    check_is_fitted(self, [\"tree_\", \"iso_kernel_\", \"n_features_in_\"])\n\n    # Process input data\n    X = check_array(X, accept_sparse=False)\n    if y is None:\n        y = np.ones(X.shape[0], dtype=np.int64)\n    else:\n        X, y = check_X_y(X, y, accept_sparse=False)\n\n    # Check feature consistency\n    if X.shape[1] != self.n_features_in_:\n        raise ValueError(\n            f\"X has {X.shape[1]} features, but STREAMKHC was trained with {self.n_features_in_} features.\"\n        )\n\n    # Transform and process data\n    X_ikv = self.iso_kernel_.transform(X, dense_output=True)\n    self._process_batch(X_ikv, y)\n    return self\n</code></pre>"},{"location":"api/stream/streakhc.html#ikpykit.stream.STREAMKHC.get_purity","title":"get_purity","text":"<pre><code>get_purity()\n</code></pre> <p>Calculate the purity of the clustering tree.</p> <p>Returns:</p> Type Description <code>float</code> <p>The purity score of the clustering tree.</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the model has not been initialized.</p> Source code in <code>ikpykit/stream/cluster/_streakhc.py</code> <pre><code>def get_purity(self) -&gt; float:\n    \"\"\"Calculate the purity of the clustering tree.\n\n    Returns\n    -------\n    float\n        The purity score of the clustering tree.\n\n    Raises\n    ------\n    NotFittedError\n        If the model has not been initialized.\n    \"\"\"\n    check_is_fitted(self, [\"tree_\"])\n    if self.tree_ is None:\n        return 0.0\n    return dendrogram_purity(self.tree_)\n</code></pre>"},{"location":"api/stream/streakhc.html#ikpykit.stream.STREAMKHC.serialize_tree","title":"serialize_tree","text":"<pre><code>serialize_tree(path)\n</code></pre> <p>Serialize the clustering tree to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save the serialized tree.</p> required <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the model has not been initialized.</p> Source code in <code>ikpykit/stream/cluster/_streakhc.py</code> <pre><code>def serialize_tree(self, path: str) -&gt; None:\n    \"\"\"Serialize the clustering tree to a file.\n\n    Parameters\n    ----------\n    path : str\n        The file path to save the serialized tree.\n\n    Raises\n    ------\n    NotFittedError\n        If the model has not been initialized.\n    \"\"\"\n    check_is_fitted(self, [\"tree_\"])\n    serliaze_tree_to_file(self.tree_, path)\n</code></pre>"},{"location":"api/stream/streakhc.html#ikpykit.stream.STREAMKHC.visualize_tree","title":"visualize_tree","text":"<pre><code>visualize_tree(path)\n</code></pre> <p>Visualize the clustering tree using Graphviz.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save the visualization.</p> required <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If the model has not been initialized.</p> Source code in <code>ikpykit/stream/cluster/_streakhc.py</code> <pre><code>def visualize_tree(self, path: str) -&gt; None:\n    \"\"\"Visualize the clustering tree using Graphviz.\n\n    Parameters\n    ----------\n    path : str\n        The file path to save the visualization.\n\n    Raises\n    ------\n    NotFittedError\n        If the model has not been initialized.\n    \"\"\"\n    check_is_fitted(self, [\"tree_\"])\n    Graphviz.write_tree(self.tree_, path)\n</code></pre>"},{"location":"api/time_series/iktod.html","title":"IKTOD","text":""},{"location":"api/time_series/iktod.html#ikpykit.timeseries.IKTOD","title":"ikpykit.timeseries.IKTOD","text":"<pre><code>IKTOD(\n    n_estimators_1=100,\n    max_samples_1=\"auto\",\n    n_estimators_2=100,\n    max_samples_2=\"auto\",\n    method=\"inne\",\n    period_length=10,\n    contamination=\"auto\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>Isolation Kernel-based Time series Subsequnce Anomaly Detection.</p> <p>IKTOD implements a distribution-based approach for anomaly time series subsequence detection. Unlike traditional time or frequency domain approaches that rely on sliding windows, IKTOD treats time series subsequences as distributions in R domain, enabling more effective similarity measurements with linear time complexity.</p> <p>This approach uses Isolation Distributional Kernel (IDK) to measure similarities between subsequences, resulting in better detection accuracy compared to sliding-window-based detectors.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators_1</code> <code>int</code> <p>Number of base estimators in the first-level ensemble.</p> <code>100</code> <code>max_samples_1</code> <code>int, float, or \"auto\"</code> <p>Number of samples for training each first-level base estimator: - int: exactly <code>max_samples_1</code> samples - float: <code>max_samples_1 * X.shape[0]</code> samples - \"auto\": <code>min(8, n_samples)</code></p> <code>\"auto\"</code> <code>n_estimators_2</code> <code>int</code> <p>Number of base estimators in the second-level ensemble.</p> <code>100</code> <code>max_samples_2</code> <code>int, float, or \"auto\"</code> <p>Number of samples for training each second-level base estimator: - int: exactly <code>max_samples_2</code> samples - float: <code>max_samples_2 * X.shape[0]</code> samples - \"auto\": <code>min(8, n_samples)</code></p> <code>\"auto\"</code> <code>method</code> <code>(inne, anne)</code> <p>Isolation method to use: - \"inne\": original Isolation Forest approach - \"anne\": approximate nearest neighbor ensemble</p> <code>\"inne\"</code> <code>period_length</code> <code>int</code> <p>Length of subsequences to split the time series.</p> <code>10</code> <code>contamination</code> <code>auto or float</code> <p>Proportion of outliers in the dataset: - \"auto\": threshold determined as in the original paper - float: must be in range (0, 0.5]</p> <code>\"auto\"</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls randomization for reproducibility.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>ikgad_</code> <code>IKGAD</code> <p>Trained Isolation Kernel Group Anomaly Detector.</p> <code>offset_</code> <code>float</code> <p>Decision threshold for outlier detection.</p> <code>is_fitted_</code> <code>bool</code> <p>Indicates if the model has been fitted.</p> References <p>.. [1] Ting, K.M., Liu, Z., Zhang, H., Zhu, Y. (2022). A New Distributional        Treatment for Time Series and An Anomaly Detection Investigation.        Proceedings of The Very Large Data Bases (VLDB) Conference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.timeseries import IKTOD\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Time series with length 40 (4 periods of length 10)\n&gt;&gt;&gt; X = np.sin(np.linspace(0, 8*np.pi, 40)).reshape(-1, 1)\n&gt;&gt;&gt; # Add anomaly\n&gt;&gt;&gt; X[25:30] = X[25:30] + 5.0\n&gt;&gt;&gt; detector = IKTOD(max_samples_1=2, max_samples_2=2, contamination=0.1, random_state=42)\n&gt;&gt;&gt; detector = detector.fit(X)\n&gt;&gt;&gt; detector.predict(X)\narray([ 1,  1, -1,  1])\n</code></pre> Source code in <code>ikpykit/timeseries/anomaly/_iktod.py</code> <pre><code>def __init__(\n    self,\n    n_estimators_1: int = 100,\n    max_samples_1: Union[int, float, str] = \"auto\",\n    n_estimators_2: int = 100,\n    max_samples_2: Union[int, float, str] = \"auto\",\n    method: str = \"inne\",\n    period_length: int = 10,\n    contamination: Union[str, float] = \"auto\",\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n):\n    self.n_estimators_1 = n_estimators_1\n    self.max_samples_1 = max_samples_1\n    self.n_estimators_2 = n_estimators_2\n    self.max_samples_2 = max_samples_2\n    self.period_length = period_length\n    self.random_state = random_state\n    self.contamination = contamination\n    self.method = method\n</code></pre>"},{"location":"api/time_series/iktod.html#ikpykit.timeseries.IKTOD.fit","title":"fit","text":"<pre><code>fit(X)\n</code></pre> <p>Fit the IKTOD model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Input time series data where: - n_samples: length of the time series - n_features: number of variables (default 1 for univariate)</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If time series length is less than or equal to period_length.</p> Source code in <code>ikpykit/timeseries/anomaly/_iktod.py</code> <pre><code>def fit(self, X) -&gt; \"IKTOD\":\n    \"\"\"Fit the IKTOD model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Input time series data where:\n        - n_samples: length of the time series\n        - n_features: number of variables (default 1 for univariate)\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If time series length is less than or equal to period_length.\n    \"\"\"\n    # Validate input data\n    X = check_array(X)\n\n    if len(X) &lt;= self.period_length:\n        raise ValueError(\n            f\"Time series length ({X.shape[0]}) must be greater than \"\n            f\"period_length ({self.period_length}).\"\n        )\n\n    # Check if time series length is compatible with period_length\n    rest_samples = X.shape[0] % self.period_length\n    if rest_samples != 0:\n        warnings.warn(\n            f\"The last sequence of series has {rest_samples} samples, \"\n            f\"which are less than other sequence.\"\n        )\n\n    # Fit the model\n    self._fit(X)\n    self.is_fitted_ = True\n\n    # Set threshold\n    if self.contamination != \"auto\":\n        if not (0.0 &lt; self.contamination &lt;= 0.5):\n            raise ValueError(\n                f\"contamination must be in (0, 0.5], got: {self.contamination}\"\n            )\n        # Define threshold based on contamination parameter\n        scores = self.score_samples(X)\n        self.offset_ = np.percentile(scores, 100.0 * self.contamination)\n    else:\n        # Use default threshold as described in the original paper\n        self.offset_ = -0.5\n\n    return self\n</code></pre>"},{"location":"api/time_series/iktod.html#ikpykit.timeseries.IKTOD.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict if subsequences contain outliers.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Time series data to evaluate</p> required <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray of shape (n_subsequences,)</code> <p>Returns +1 for inliers and -1 for outliers for each subsequence.</p> Source code in <code>ikpykit/timeseries/anomaly/_iktod.py</code> <pre><code>def predict(self, X) -&gt; np.ndarray:\n    \"\"\"Predict if subsequences contain outliers.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Time series data to evaluate\n\n    Returns\n    -------\n    labels : ndarray of shape (n_subsequences,)\n        Returns +1 for inliers and -1 for outliers for each subsequence.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    X = check_array(X)\n    X_sep = self._split_to_subsequences(X)\n    return self.ikgad_.predict(X_sep)\n</code></pre>"},{"location":"api/time_series/iktod.html#ikpykit.timeseries.IKTOD.decision_function","title":"decision_function","text":"<pre><code>decision_function(X)\n</code></pre> <p>Compute decision scores for subsequences.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Time series data to evaluate</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_subsequences,)</code> <p>Decision scores. Negative scores represent outliers, positive scores represent inliers.</p> Source code in <code>ikpykit/timeseries/anomaly/_iktod.py</code> <pre><code>def decision_function(self, X) -&gt; np.ndarray:\n    \"\"\"Compute decision scores for subsequences.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Time series data to evaluate\n\n    Returns\n    -------\n    scores : ndarray of shape (n_subsequences,)\n        Decision scores. Negative scores represent outliers,\n        positive scores represent inliers.\n    \"\"\"\n    return self.score_samples(X) - self.offset_\n</code></pre>"},{"location":"api/time_series/iktod.html#ikpykit.timeseries.IKTOD.score_samples","title":"score_samples","text":"<pre><code>score_samples(X)\n</code></pre> <p>Compute anomaly scores for subsequences.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_samples, n_features)</code> <p>Time series data to evaluate</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_subsequences,)</code> <p>Anomaly scores where lower values indicate more anomalous subsequences.</p> Source code in <code>ikpykit/timeseries/anomaly/_iktod.py</code> <pre><code>def score_samples(self, X) -&gt; np.ndarray:\n    \"\"\"Compute anomaly scores for subsequences.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Time series data to evaluate\n\n    Returns\n    -------\n    scores : ndarray of shape (n_subsequences,)\n        Anomaly scores where lower values indicate more anomalous subsequences.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    X = check_array(X)\n    X_sep = self._split_to_subsequences(X)\n    return self.ikgad_.score_samples(X_sep)\n</code></pre>"},{"location":"api/trajectory/ikat.html","title":"IKAT","text":""},{"location":"api/trajectory/ikat.html#ikpykit.trajectory.IKAT","title":"ikpykit.trajectory.IKAT","text":"<pre><code>IKAT(\n    n_estimators_1=100,\n    max_samples_1=\"auto\",\n    n_estimators_2=100,\n    max_samples_2=\"auto\",\n    contamination=\"auto\",\n    method=\"inne\",\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>OutlierMixin</code>, <code>BaseEstimator</code></p> <p>Isolation-based anomaly detection for trajectory data.</p> <p>IKAT is a trajectory anomaly detection algorithm that leverages the Isolation Distribution Kernel. Trajectory data is a sequence of points in a multi-dimensional space. It leverages a two-step approach: first transforming the data using an isolation kernel, then calculating kernel mean embeddings for each trajectory data to detect anomalous trajectory. The algorithm is effective for detecting both global and local trajectory anomalies.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators_1</code> <code>int</code> <p>Number of base estimators in the first step ensemble.</p> <code>200</code> <code>max_samples_1</code> <code>(int, float or auto)</code> <p>Number of samples to draw for training each base estimator in first step: - If int, draws exactly <code>max_samples_1</code> samples - If float, draws <code>max_samples_1 * n_samples</code> samples - If \"auto\", draws <code>min(8, n_samples)</code> samples</p> <code>\"auto\"</code> <code>n_estimators_2</code> <code>int</code> <p>Number of base estimators in the second step ensemble.</p> <code>200</code> <code>max_samples_2</code> <code>(int, float or auto)</code> <p>Number of samples to draw for training each base estimator in second step: - If int, draws exactly <code>max_samples_2</code> samples - If float, draws <code>max_samples_2 * n_samples</code> samples - If \"auto\", draws <code>min(8, n_samples)</code> samples</p> <code>\"auto\"</code> <code>method</code> <code>(inne, anne)</code> <p>Isolation method to use. \"inne\" is the original algorithm from the paper.</p> <code>\"inne\"</code> <code>contamination</code> <code>auto or float</code> <p>Proportion of outliers in the dataset: - If \"auto\", threshold is determined as in the original paper - If float, must be in range (0, 0.5]</p> <code>\"auto\"</code> <code>random_state</code> <code>(int, RandomState or None)</code> <p>Controls randomness for reproducibility.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>offset_</code> <code>float</code> <p>Offset used to define the decision function from the raw scores.</p> <code>ikgod_</code> <code>IKGAD</code> <p>The fitted IKGAD object.</p> <code>is_fitted_</code> <code>bool</code> <p>Flag indicating if the estimator is fitted.</p> References <p>.. [1] Wang, Y., Wang, Z., Ting, K. M., &amp; Shang, Y. (2024).    A Principled Distributional Approach to Trajectory Similarity Measurement and    its Application to Anomaly Detection. Journal of Artificial Intelligence Research, 79, 865-893.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.trajectory import IKAT\n&gt;&gt;&gt; from ikpykit.trajectory.dataloader import SheepDogs\n&gt;&gt;&gt; sheepdogs = SheepDogs()\n&gt;&gt;&gt; X, y = sheepdogs.load(return_X_y=True)\n&gt;&gt;&gt; clf = IKAT().fit(X)\n&gt;&gt;&gt; predictions = clf.predict(X)\n&gt;&gt;&gt; anomaly_scores = clf.score_samples(X)\n</code></pre> Source code in <code>ikpykit/trajectory/anomaly/_ikat.py</code> <pre><code>def __init__(\n    self,\n    n_estimators_1: int = 100,\n    max_samples_1: Union[int, float, str] = \"auto\",\n    n_estimators_2: int = 100,\n    max_samples_2: Union[int, float, str] = \"auto\",\n    contamination: Union[str, float] = \"auto\",\n    method: Literal[\"inne\", \"anne\", \"auto\"] = \"inne\",\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n):\n    self.n_estimators_1 = n_estimators_1\n    self.max_samples_1 = max_samples_1\n    self.n_estimators_2 = n_estimators_2\n    self.max_samples_2 = max_samples_2\n    self.random_state = random_state\n    self.contamination = contamination\n    self.method = method\n</code></pre>"},{"location":"api/trajectory/ikat.html#ikpykit.trajectory.IKAT.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the anomaly detector.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_trajectories, n_points, n_features)</code> <p>The input trajectories to train on.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If contamination is outside of (0, 0.5] range or method is not valid.</p> Source code in <code>ikpykit/trajectory/anomaly/_ikat.py</code> <pre><code>def fit(self, X: list, y: Any = None) -&gt; \"IKAT\":\n    \"\"\"Fit the anomaly detector.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_trajectories, n_points, n_features)\n        The input trajectories to train on.\n\n    y : Ignored\n        Not used, present for API consistency.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If contamination is outside of (0, 0.5] range or method is not valid.\n    \"\"\"\n    X = check_format(X, n_features=2)\n\n    # Validate method parameter\n    if self.method not in [\"inne\", \"anne\"]:\n        raise ValueError(\n            f\"method must be one of 'inne', 'anne', got: {self.method}\"\n        )\n\n    # Validate contamination parameter\n    if self.contamination != \"auto\" and not (0.0 &lt; self.contamination &lt;= 0.5):\n        raise ValueError(\n            f\"contamination must be in (0, 0.5], got: {self.contamination}\"\n        )\n\n    # Fit the model\n    self._fit(X)\n    self.is_fitted_ = True\n\n    # Set the offset for decision function\n    if self.contamination == \"auto\":\n        self.offset_ = -0.5\n    else:\n        # Set threshold based on contamination level\n        self.offset_ = np.percentile(\n            self.score_samples(X), 100.0 * self.contamination\n        )\n\n    return self\n</code></pre>"},{"location":"api/trajectory/ikat.html#ikpykit.trajectory.IKAT.predict","title":"predict","text":"<pre><code>predict(X)\n</code></pre> <p>Predict if trajectories are outliers or not.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_trajectories, n_points, n_features)</code> <p>The input trajectories.</p> required <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray of shape (n_trajectories,)</code> <p>The predicted labels: - 1 for inliers - -1 for outliers</p> Source code in <code>ikpykit/trajectory/anomaly/_ikat.py</code> <pre><code>def predict(self, X: list) -&gt; list:\n    \"\"\"Predict if trajectories are outliers or not.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_trajectories, n_points, n_features)\n        The input trajectories.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_trajectories,)\n        The predicted labels:\n        - 1 for inliers\n        - -1 for outliers\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    X = check_format(X, n_features=2)\n    return self.ikgad_.predict(X)\n</code></pre>"},{"location":"api/trajectory/ikat.html#ikpykit.trajectory.IKAT.decision_function","title":"decision_function","text":"<pre><code>decision_function(X)\n</code></pre> <p>Compute the decision function for each trajectory.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_trajectories, n_points, n_features)</code> <p>The input trajectories.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_trajectories,)</code> <p>The decision function value for each trajectory. Negative values indicate outliers, positive values indicate inliers.</p> Source code in <code>ikpykit/trajectory/anomaly/_ikat.py</code> <pre><code>def decision_function(self, X: list) -&gt; list:\n    \"\"\"Compute the decision function for each trajectory.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_trajectories, n_points, n_features)\n        The input trajectories.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_trajectories,)\n        The decision function value for each trajectory.\n        Negative values indicate outliers, positive values indicate inliers.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    X = check_format(X, n_features=2)  # Add format check for consistency\n    return self.ikgad_.decision_function(X)\n</code></pre>"},{"location":"api/trajectory/ikat.html#ikpykit.trajectory.IKAT.score_samples","title":"score_samples","text":"<pre><code>score_samples(X)\n</code></pre> <p>Compute the anomaly scores for each trajectory.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_trajectories, n_points, n_features)</code> <p>The input trajectories.</p> required <p>Returns:</p> Name Type Description <code>scores</code> <code>ndarray of shape (n_trajectories,)</code> <p>The anomaly scores for each trajectory. Lower scores indicate more anomalous trajectories.</p> Source code in <code>ikpykit/trajectory/anomaly/_ikat.py</code> <pre><code>def score_samples(self, X: list) -&gt; list:\n    \"\"\"Compute the anomaly scores for each trajectory.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_trajectories, n_points, n_features)\n        The input trajectories.\n\n    Returns\n    -------\n    scores : ndarray of shape (n_trajectories,)\n        The anomaly scores for each trajectory.\n        Lower scores indicate more anomalous trajectories.\n    \"\"\"\n    check_is_fitted(self, \"is_fitted_\")\n    X = check_format(X, n_features=2)  # Use check_format consistently\n\n    return self.ikgad_.score_samples(X)\n</code></pre>"},{"location":"api/trajectory/tidkc.html","title":"TIDKC","text":""},{"location":"api/trajectory/tidkc.html#ikpykit.trajectory.TIDKC","title":"ikpykit.trajectory.TIDKC","text":"<pre><code>TIDKC(\n    k,\n    kn,\n    v,\n    n_init_samples,\n    n_estimators_1=100,\n    max_samples_1=\"auto\",\n    n_estimators_2=100,\n    max_samples_2=\"auto\",\n    method=\"anne\",\n    is_post_process=True,\n    random_state=None,\n)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClusterMixin</code></p> <p>Trajectory Isolation Distributional Kernel Clustering (TIDKC).</p> <p>TIDKC identifies non-linearly separable clusters with irregular shapes and varied densities in trajectory data using distributional kernels. It operates in linear time, does not rely on random initialization, and is robust to outliers.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of clusters to form.</p> required <code>kn</code> <code>int</code> <p>The number of nearest neighbors to consider when calculating the local contrast.</p> required <code>v</code> <code>float</code> <p>The decay factor for reducing the threshold value.</p> required <code>n_init_samples</code> <code>int</code> <p>The number of samples to use for initializing the cluster centers.</p> required <code>n_estimators_1</code> <code>int</code> <p>Number of base estimators in the first step ensemble.</p> <code>100</code> <code>max_samples_1</code> <code>(int, float or auto)</code> <p>Number of samples to draw for training each base estimator in first step: - If int, draws exactly <code>max_samples_1</code> samples - If float, draws <code>max_samples_1 * n_samples</code> samples - If \"auto\", draws <code>min(8, n_samples)</code> samples</p> <code>\"auto\"</code> <code>n_estimators_2</code> <code>int</code> <p>Number of base estimators in the second step ensemble.</p> <code>100</code> <code>max_samples_2</code> <code>(int, float or auto)</code> <p>Number of samples to draw for training each base estimator in second step: - If int, draws exactly <code>max_samples_2</code> samples - If float, draws <code>max_samples_2 * n_samples</code> samples - If \"auto\", draws <code>min(8, n_samples)</code> samples</p> <code>\"auto\"</code> <code>method</code> <code>(inne, anne)</code> <p>Isolation method to use. \"anne\" is the original algorithm from the paper.</p> <code>\"inne\"</code> <code>is_post_process</code> <code>bool</code> <p>Whether to perform post-processing to refine the clusters.</p> <code>True</code> <code>random_state</code> <code>int, RandomState instance or None</code> <p>Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>labels_</code> <code>ndarray of shape (n_samples,)</code> <p>Cluster labels for each point in the dataset.</p> <code>iso_kernel_</code> <code>IsoKernel</code> <p>The fitted isolation kernel.</p> <code>idkc_</code> <code>IDKC</code> <p>The fitted IDKC clustering model.</p> References <p>.. [1] Z. J. Wang, Y. Zhu and K. M. Ting, \"Distribution-Based Trajectory Clustering,\"        2023 IEEE International Conference on Data Mining (ICDM).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.trajectory import TIDKC\n&gt;&gt;&gt; from ikpykit.trajectory.dataloader import SheepDogs\n&gt;&gt;&gt; sheepdogs = SheepDogs()\n&gt;&gt;&gt; X, y = sheepdogs.load(return_X_y=True)\n&gt;&gt;&gt; clf = TIDKC(k=2, kn=5, v=0.5, n_init_samples=10).fit(X)\n&gt;&gt;&gt; predictions = clf.fit_predict(X)\n</code></pre> Source code in <code>ikpykit/trajectory/cluster/_tidkc.py</code> <pre><code>def __init__(\n    self,\n    k: int,\n    kn: int,\n    v: float,\n    n_init_samples: int,\n    n_estimators_1: int = 100,\n    max_samples_1: Union[int, float, str] = \"auto\",\n    n_estimators_2: int = 100,\n    max_samples_2: Union[int, float, str] = \"auto\",\n    method: Literal[\"inne\", \"anne\"] = \"anne\",\n    is_post_process: bool = True,\n    random_state: Optional[Union[int, np.random.RandomState]] = None,\n):\n    self.n_estimators_1 = n_estimators_1\n    self.max_samples_1 = max_samples_1\n    self.n_estimators_2 = n_estimators_2\n    self.max_samples_2 = max_samples_2\n    self.method = method\n    self.k = k\n    self.kn = kn\n    self.v = v\n    self.n_init_samples = n_init_samples\n    self.is_post_process = is_post_process\n    self.random_state = random_state\n</code></pre>"},{"location":"api/trajectory/tidkc.html#ikpykit.trajectory.TIDKC.fit","title":"fit","text":"<pre><code>fit(X, y=None)\n</code></pre> <p>Fit the trajectory cluster model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_trajectories, n_points, n_features)</code> <p>The input trajectories to train on.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If method is not valid.</p> Source code in <code>ikpykit/trajectory/cluster/_tidkc.py</code> <pre><code>def fit(self, X: list, y: Any = None) -&gt; \"TIDKC\":\n    \"\"\"Fit the trajectory cluster model.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_trajectories, n_points, n_features)\n        The input trajectories to train on.\n\n    y : Ignored\n        Not used, present for API consistency.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n\n    Raises\n    ------\n    ValueError\n        If method is not valid.\n    \"\"\"\n    X = check_format(X, n_features=2)\n\n    # Validate method parameter\n    if self.method not in [\"inne\", \"anne\"]:\n        raise ValueError(\n            f\"method must be one of 'inne', 'anne', got: {self.method}\"\n        )\n\n    # Fit the model\n    self._fit(X)\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"api/trajectory/tidkc.html#ikpykit.trajectory.TIDKC.fit_predict","title":"fit_predict","text":"<pre><code>fit_predict(X, y=None)\n</code></pre> <p>Fit the model and predict clusters for X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like of shape (n_trajectories, n_points, n_features)</code> <p>The input trajectories.</p> required <code>y</code> <code>Ignored</code> <p>Not used, present for API consistency.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>labels</code> <code>ndarray of shape (n_samples,)</code> <p>Cluster labels.</p> Source code in <code>ikpykit/trajectory/cluster/_tidkc.py</code> <pre><code>def fit_predict(self, X, y=None):\n    \"\"\"Fit the model and predict clusters for X.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_trajectories, n_points, n_features)\n        The input trajectories.\n\n    y : Ignored\n        Not used, present for API consistency.\n\n    Returns\n    -------\n    labels : ndarray of shape (n_samples,)\n        Cluster labels.\n    \"\"\"\n    return super().fit_predict(X, y)\n</code></pre>"},{"location":"api/trajectory/data_loader/sheep_dogs.html","title":"SheepDogs","text":""},{"location":"api/trajectory/data_loader/sheep_dogs.html#ikpykit.trajectory.dataloader.SheepDogs","title":"ikpykit.trajectory.dataloader.SheepDogs","text":"<pre><code>SheepDogs()\n</code></pre> <p>               Bases: <code>FileDataset</code></p> <p>SheepDogs trajectory dataset.</p> <p>A trajectory dataset collected from MoveBank containing movement patterns of sheep dogs and other related animals. This dataset can be used for trajectory analysis and anomaly detection.</p> <p>The dataset is loaded with 2 features (longitude and latitude), and samples are classified into 2 classes (normal and anomalous).</p> <p>Attributes:</p> Name Type Description <code>n_features</code> <code>int</code> <p>Number of features in the dataset (2: longitude and latitude).</p> <code>n_samples</code> <code>int</code> <p>Total number of trajectory samples after processing.</p> <code>n_classes</code> <code>int</code> <p>Number of classes (2: normal and anomalous).</p> <code>anomaly_ratio</code> <code>float</code> <p>Ratio of anomalous trajectories to total trajectories.</p> References <p>.. [1] Movebank: https://www.movebank.org/cms/movebank-main</p> <p>.. [2] Wang, Y., Wang, Z., Ting, K. M., &amp; Shang, Y. (2024).    A Principled Distributional Approach to Trajectory Similarity Measurement and    its Application to Anomaly Detection. Journal of Artificial Intelligence Research, 79, 865-893.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from ikpykit.trajectory.dataloader import SheepDogs\n&gt;&gt;&gt; sheepdogs = SheepDogs()\n&gt;&gt;&gt; X, y = sheepdogs.load(return_X_y=True)\n</code></pre> Source code in <code>ikpykit/trajectory/dataloader/_sheepdogs.py</code> <pre><code>def __init__(self):\n    super().__init__(\n        n_features=2,\n        n_samples=None,\n        n_classes=2,\n        filename=\"./datasets/sheepdogs.zip\",\n    )\n    self.anomaly_ratio = None\n</code></pre>"},{"location":"api/trajectory/data_loader/sheep_dogs.html#ikpykit.trajectory.dataloader.SheepDogs.load","title":"load","text":"<pre><code>load(return_X_y=False)\n</code></pre> <p>Load the SheepDogs dataset.</p> <p>Parameters:</p> Name Type Description Default <code>return_X_y</code> <code>bool</code> <p>If True, returns a tuple (X, y) where X is the data and y is the target. If False, returns a dict with keys 'X' and 'y'.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict or tuple</code> <p>Either (X, y) tuple or {'X': data, 'y': target} dict where data is a list of trajectories and target indicates normal (1) or anomalous (0) trajectories.</p> Source code in <code>ikpykit/trajectory/dataloader/_sheepdogs.py</code> <pre><code>def load(self, return_X_y=False):\n    \"\"\"Load the SheepDogs dataset.\n\n    Parameters\n    ----------\n    return_X_y : bool, default=False\n        If True, returns a tuple (X, y) where X is the data and y is the target.\n        If False, returns a dict with keys 'X' and 'y'.\n\n    Returns\n    -------\n    dict or tuple\n        Either (X, y) tuple or {'X': data, 'y': target} dict where data is a list\n        of trajectories and target indicates normal (1) or anomalous (0) trajectories.\n    \"\"\"\n    # Load and filter data\n    data = pd.read_csv(\n        self.path,\n        usecols=[\n            \"timestamp\",\n            \"location-long\",\n            \"location-lat\",\n            \"individual-local-identifier\",\n        ],\n        parse_dates=[\"timestamp\"],\n    )\n\n    # Select specific individuals for dataset\n    individual_local = data[\"individual-local-identifier\"].value_counts().index\n    selected_individuals = [\n        0,\n        1,\n        2,\n        6,\n        14,\n    ]  # Individual #14 is considered anomalous\n    mask = data[\"individual-local-identifier\"].isin(\n        individual_local[selected_individuals]\n    )\n    sub_data = data[mask]\n    sub_data.dropna(\n        subset=[\"location-long\", \"location-lat\"], how=\"any\", inplace=True\n    )\n\n    # Define trajectory splitting parameters\n    gap = datetime.timedelta(hours=1)\n    tmp_traj = []\n    anomalies = []\n    normal_traj = []\n    labels = []\n\n    # Process trajectories\n    for i in range(1, len(sub_data)):\n        previous_location = [\n            sub_data[\"location-long\"].iloc[i - 1],\n            sub_data[\"location-lat\"].iloc[i - 1],\n        ]\n        previous_timestamp = sub_data[\"timestamp\"].iloc[i - 1]\n        current_timestamp = sub_data[\"timestamp\"].iloc[i]\n        previous_individual = sub_data[\"individual-local-identifier\"].iloc[i - 1]\n        current_individual = sub_data[\"individual-local-identifier\"].iloc[i]\n        tmp_traj.append(previous_location)\n\n        # Split trajectory when there's a large time gap or different individual\n        if (\n            current_timestamp - previous_timestamp &gt; gap\n            or previous_individual != current_individual\n        ):\n            if len(tmp_traj) &gt; 10:  # Ensure minimum trajectory length\n                if previous_individual == individual_local[14]:\n                    anomalies.append(tmp_traj)\n                    labels.append(0)  # 0 for anomalous\n                else:\n                    normal_traj.append(tmp_traj)\n                    labels.append(1)  # 1 for normal\n            tmp_traj = []\n\n    # Filter anomalies with a minimum length requirement\n    filtered_anomalies = [x for x in anomalies if len(x) &gt; 16]\n    all_traj = normal_traj + filtered_anomalies\n\n    # Update object attributes\n    self.anomaly_ratio = len(filtered_anomalies) / len(all_traj)\n    self.n_samples = len(all_traj)\n\n    if return_X_y:\n        return all_traj, labels\n    else:\n        return {\n            \"X\": all_traj,\n            \"y\": labels,\n        }\n</code></pre>"},{"location":"authors/authors.html","title":"Authors","text":"<p>All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.</p> <p>A detailed overview on how to contribute can be found in the contributing guide.</p>"},{"location":"authors/authors.html#core-development-team","title":"Core Development Team","text":"<ul> <li> <p>Xin Han (xhan197@outlook.com)</p> </li> <li> <p>Yixiao Ma (mayx@lamda.nju.edu.cn)</p> </li> <li> <p>Ye Zhu (ye.zhu@ieee.org)</p> </li> <li> <p>Kaiming Ting (tingkm@nju.edu.cn)</p> </li> </ul>"},{"location":"authors/authors.html#main-contributors","title":"Main Contributors","text":"<ul> <li>Xin han, Xin Han</li> <li>Yixiao Ma, Yixiao Ma</li> </ul>"},{"location":"authors/authors.html#contributions","title":"Contributions","text":""},{"location":"contributing/contribution.html","title":"Contributing to ikpykit","text":"<p>Hi! Thanks for your interest in contributing to ikpykit :D . In this document we'll try to summarize everything that you need to know to do a good job.</p>"},{"location":"contributing/contribution.html#code-and-issues","title":"Code and Issues","text":"<p>We use Github to host our code repositories and issues.You can look at issues to report any issues related to ikpykit. Here is a guide on how to report better issues.</p>"},{"location":"contributing/contribution.html#git-and-our-branching-model","title":"Git and our Branching model","text":""},{"location":"contributing/contribution.html#git","title":"Git","text":"<p>We use Git as our version control system, so the best way to contribute is to learn how to use it and put your changes on a Git repository. There are plenty of documentation about Git -- you can start with the Pro Git book. Or You can go through the try git tutorial.</p>"},{"location":"contributing/contribution.html#forks-github-pull-requests","title":"Forks + GitHub Pull requests","text":"<p>We use the famous gitflow to manage our branches.</p> <p>Summary of our git branching model:</p> <ul> <li>Fork the desired repository on GitHub to your account;</li> <li>Clone your forked repository locally: <code>git clone git@github.com:your-username:repository-name.git</code>;</li> <li>Create a new branch off of <code>develop</code> with a descriptive name (for example: <code>feature/portuguese-sentiment-analysis</code>, <code>hotfix/bug-on-downloader</code>). You can do it by switching to <code>develop</code> branch (<code>git checkout develop</code>) and then creating a new branch (<code>git checkout -b name-of-the-new-branch</code>);</li> <li>Do many small commits on that branch locally (<code>git add files-changed</code>, <code>git commit -m \"Add some change\"</code>);</li> <li>Push to your fork on GitHub (with the name as your local branch: <code>git push origin branch-name</code>);</li> <li>Create a pull request using the GitHub Web interface (asking us to pull the changes from your new branch and add the changes to our <code>develop</code> branch);</li> <li>Wait for comments.</li> </ul>"},{"location":"contributing/contribution.html#tips","title":"Tips","text":"<ul> <li>Write helpful commit messages.</li> <li>Anything in the <code>dev</code> branch should be deployable (no failing tests).</li> <li>Never use <code>git add .</code>: it can add unwanted files;</li> <li>Avoid using <code>git commit -a</code> unless you know what you're doing;</li> <li>Check every change with <code>git diff</code> before adding then to the index (stage area) and with <code>git diff --cached</code> before commiting;</li> <li>If you have push access to the main repository, please do not commit directly to <code>dev</code>: your access should be used only to accept pull requests; if you want to make a new feature, you should use the same process as other developers so that your code can be reviewed.</li> </ul>"},{"location":"contributing/contribution.html#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"contributing/contribution.html#code-guidelines","title":"Code Guidelines","text":"<ul> <li>We use PEP8;</li> <li>We permit 120 characters in a line, rather 79 as suggested in PEP8</li> <li>Write tests for your new features (please see \"Tests\" topic below);</li> <li>Always remember that commented code is dead code;</li> <li>Name identifiers (variables, classes, functions, module names) with readable names (<code>x</code> is always wrong);</li> <li>When manipulating strings, use Python's new-style formatting (<code>'{} = {}'.format(a, b)</code> instead of <code>'%s = %s' % (a, b)</code>);</li> <li>When working with files use <code>with open(&lt;filename&gt;,(option&gt;) as f</code> instead of <code>f = open(&lt;filename&gt;,(option&gt;)</code>;</li> <li>Run all tests before pushing <code>pytest</code> so you will know if your changes broke something;</li> </ul>"},{"location":"contributing/contribution.html#tests","title":"Tests","text":"<p>We use Github Actionsfor continous integration and python pytest for writing tests. You should write tests for every feature you add or bug you solve in the code. Having automated tests for every line of our code let us make big changes without worries: there will always be tests to verify if the changes introduced bugs or lack of features. If we don't have tests we will be blind and every change will come with some fear of possibly breaking something.</p> <p>For a better design of your code, we recommend using a technique called test-driven development, where you write your tests before writing the actual code that implements the desired feature.</p>"},{"location":"contributing/contribution.html#discussion","title":"Discussion","text":"<p>Please feel free to contact us through mail list if you have any questions or suggestions. Every contribution is very welcome!</p> <p>Happy hacking! ;)</p>"},{"location":"examples/examples_english.html","title":"Examples","text":""},{"location":"examples/examples_english.html#examples-and-tutorials","title":"Examples and Tutorials","text":"<p>Practical examples and tutorials to help you understand and apply ikpykit.</p> <p>Anomaly Detection</p> <ul> <li>INNE</li> </ul>"},{"location":"faq/table-of-contents.html","title":"Frequently Asked Questions and forecasting tips","text":"<p>Thank you for choosing skforecast and visiting our Frequently Asked Questions (FAQ) page. Here, we aim to provide solutions to commonly encountered issues on our Github repository. If your question is not answered on this page, we encourage you to create a new issue on our Github so that it can be addressed, and other users can also benefit from it. Additionally, we have included some forecasting tips to help you get the most out of skforecast.</p> <p>General Forecasting Tips</p> <ul> <li>Avoid negative predictions when forecasting</li> <li>Forecasting time series with missing values</li> <li>Forecasting with delayed historical data</li> <li>Backtesting vs One-step-ahead</li> </ul> <p>Feature Engineering</p> <ul> <li>Cyclical features in time series</li> <li>Time series aggregation</li> </ul> <p>Performance Optimization</p> <ul> <li>Parallelization in skforecast</li> <li>Profiling skforecast</li> </ul>"},{"location":"quick-start/how-to-install.html","title":"Installation Guide","text":"<p>This guide will help you install <code>ikpykit</code>. The default installation of <code>ikpykit</code> includes only the essential dependencies required for basic functionality. Additional optional dependencies can be installed for extended features.</p> <p> </p>"},{"location":"quick-start/how-to-install.html#basic-installation","title":"Basic installation","text":"<p>To install the basic version of <code>ikpykit</code> with its core dependencies, run:</p> <pre><code>pip install ikpykit\n</code></pre> <p>If you're feeling brave, feel free to install the bleeding edge: NOTE: Do so at your own risk; no guarantees given! Latest (unstable):</p> <pre><code>pip install git+https://github.com/IsolationKernel/ikpykit.git@main --upgrade\n</code></pre> <p>Once the installation is completed, you can check whether the installation was successful through:</p> <pre><code>import ikpyikt\nprint(ikpyikt.__version__)\n</code></pre>"},{"location":"quick-start/how-to-install.html#dependencies","title":"Dependencies","text":"<p>The following dependencies are installed with the default installation:</p> <ul> <li>numpy&gt;=1.22</li> <li>pandas&gt;=1.5</li> <li>tqdm&gt;=4.57</li> <li>scikit-learn&gt;=1.2</li> <li>joblib&gt;=1.1</li> <li>numba&gt;=0.59</li> </ul>"},{"location":"releases/releases.html","title":"Changelog","text":"<p>All significant changes to this project are documented in this release file.</p> Legend Feature New feature Enhancement Improvement in existing functionality API Change Changes in the API Fix Bug fix"},{"location":"user_guides/inne.html","title":"INNE","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom ikpykit.anomaly import IsolationNNE\n\nrng = np.random.RandomState(42)\n\n# Generate train data\nX = 0.3 * rng.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * rng.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = IsolationNNE()\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\n\n# plot the line, the samples, and the nearest vectors to the plane\nxx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title(\"IsolationNNE\")\nplt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=20, edgecolor=\"k\")\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\"green\", s=20, edgecolor=\"k\")\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"red\", s=20, edgecolor=\"k\")\nplt.axis(\"tight\")\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend(\n    [b1, b2, c],\n    [\"training observations\", \"new regular observations\", \"new abnormal observations\"],\n    loc=\"upper left\",\n)\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from ikpykit.anomaly import IsolationNNE  rng = np.random.RandomState(42)  # Generate train data X = 0.3 * rng.randn(100, 2) X_train = np.r_[X + 2, X - 2] # Generate some regular novel observations X = 0.3 * rng.randn(20, 2) X_test = np.r_[X + 2, X - 2] # Generate some abnormal novel observations X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))  # fit the model clf = IsolationNNE() clf.fit(X_train) y_pred_train = clf.predict(X_train) y_pred_test = clf.predict(X_test) y_pred_outliers = clf.predict(X_outliers)  # plot the line, the samples, and the nearest vectors to the plane xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50)) Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)  plt.title(\"IsolationNNE\") plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)  b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\"white\", s=20, edgecolor=\"k\") b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\"green\", s=20, edgecolor=\"k\") c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\"red\", s=20, edgecolor=\"k\") plt.axis(\"tight\") plt.xlim((-5, 5)) plt.ylim((-5, 5)) plt.legend(     [b1, b2, c],     [\"training observations\", \"new regular observations\", \"new abnormal observations\"],     loc=\"upper left\", ) plt.show()"},{"location":"user_guides/inne.html#isolationnne-example","title":"IsolationNNE example\u00b6","text":"<p>An example using :class:<code>ikpykit.anomaly.IsolationNNE</code> for anomaly detection.</p>"},{"location":"user_guides/table-of-contents.html","title":"Table of Contents","text":"<p>Welcome to the ikpykit user guides! This comprehensive collection of guides is designed to help you navigate through the various features and functionalities of ikpykit. Whether you are a beginner or an advanced user, you will find the necessary resources to master data with ikpykit. Below, you will find the user guides categorized by topic for easier navigation.</p> <p>Anomaly Detection</p> <ul> <li>INNE</li> </ul> <p>We hope you find these guides helpful. If you have any questions or need further assistance, please don't hesitate to reach out to the ikpykit community.</p>"}]}